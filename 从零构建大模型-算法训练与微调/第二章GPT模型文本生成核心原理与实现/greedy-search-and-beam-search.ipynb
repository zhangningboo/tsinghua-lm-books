{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08077770",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'v11 (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n v11 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380d10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制\n",
    "class GPT2SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, dropout=0.1):\n",
    "        super(GPT2SelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embedding_size\n",
    "        ), \"Embedding size needs to be divisible by num_heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embedding_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, values, keys, queries):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # 分割嵌入向量为多个头\n",
    "        values = values.view(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / (self.head_dim ** 0.5)\n",
    "\n",
    "        mask = torch.tril(torch.ones((query_len, key_len))).expand(N, self.num_heads, query_len, key_len)\n",
    "        energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).view(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b596105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dim, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b076397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2基础块定义\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, hidden_dim, dropout=0.1):\n",
    "        super(GPT2Block, self).__init__()\n",
    "        self.attention = GPT2SelfAttention(embedding_size, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = FeedForward(embedding_size, hidden_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 自注意力子层\n",
    "        attn_output = self.attention(x, x, x)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        norm1_out = self.norm1(x + attn_output)\n",
    "\n",
    "        # 前馈神经网络子层\n",
    "        ffn_output = self.feed_forward(norm1_out)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        norm2_out = self.norm2(norm1_out + ffn_output)\n",
    "        return norm2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dade69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GPT-2模型的基础生成模块\n",
    "class GPT2TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_layers, num_heads, hidden_dim, max_length=50, dropout=0.1):\n",
    "        super(GPT2TextGenerator, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GPT2Block(embedding_size, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        N, seq_length = input_ids.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(input_ids.device)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, start_token, max_len, temperature=1.0):\n",
    "        generated = start_token\n",
    "        for _ in range(max_len - len(start_token)):\n",
    "            x = torch.tensor(generated).unsqueeze(0).to(next(self.parameters()).device)\n",
    "            logits = self.forward(x)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probs, dim=-1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == 0:  # 假设0是结束标记\n",
    "                break\n",
    "        return generated\n",
    "\n",
    "    def greedy_search(self, start_token):\n",
    "        generated = start_token\n",
    "        for _ in range(self.max_length - len(start_token)):\n",
    "            x = torch.tensor(generated).unsqueeze(0).to(next(self.parameters()).device)\n",
    "            logits = self.forward(x)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == 0:  # 假设0是结束标记\n",
    "                break\n",
    "        return generated\n",
    "\n",
    "    def beam_search(self, start_token, beam_width=3):\n",
    "        sequences = [(start_token, 0)]\n",
    "        \n",
    "        for _ in range(self.max_length - len(start_token)):\n",
    "            all_candidates = []\n",
    "            for seq, score in sequences:\n",
    "                x = torch.tensor(seq).unsqueeze(0).to(next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                next_token_logits = logits[:, -1, :]\n",
    "                probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                topk_probs, topk_indices = torch.topk(probs, beam_width, dim=-1)\n",
    "                \n",
    "                for k in range(beam_width):\n",
    "                    candidate = seq + [topk_indices[0][k].item()]\n",
    "                    candidate_score = score + topk_probs[0][k].item()\n",
    "                    all_candidates.append((candidate, candidate_score))\n",
    "            # 选择得分最高的beam_width个序列\n",
    "            ordered = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)\n",
    "            sequences = ordered[:beam_width]\n",
    "            # 检查是否所有序列都以结束标记结尾\n",
    "            if any(seq[-1] == 0 for seq, _ in sequences):\n",
    "                break\n",
    "        # 选择得分最高的序列\n",
    "        return max(sequences, key=lambda tup: tup[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79611a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "vocab_size = 50527\n",
    "embedding_size = 768\n",
    "num_layers = 12\n",
    "heads = 12\n",
    "hidden_dim = 3072\n",
    "max_len = 20\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61096782",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 初始化模型\n",
    "gpt2_model = GPT2TextGenerator(vocab_size, embedding_size, num_layers, num_heads=heads, hidden_dim=hidden_dim, max_length=max_len, dropout=dropout)\n",
    "gpt2_model = gpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60df6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟提示次输入\n",
    "start_token = [1, 345, 876]\n",
    "start_token = torch.tensor(start_token).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search生成文本\n",
    "greedy_generated_sequence = gpt2_model.greedy_search(start_token.tolist())\n",
    "print(\"Greedy Search生成的文本序列:\", greedy_generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search生成的文本\n",
    "beam_generated_sequence = gpt2_model.beam_search(start_token.tolist())\n",
    "print(\"Beam Search生成的文本序列:\", beam_generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362acfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟解码生成的文本\n",
    "def decode_sequence(sequence):\n",
    "    return \" \".join([f\"<token_{idx}>\" for idx in sequence])\n",
    "\n",
    "greedy_decoded_text = decode_sequence(greedy_generated_sequence)\n",
    "beam_decoded_text = decode_sequence(beam_generated_sequence)\n",
    "\n",
    "print(\"Greedy Search解码生成的文本序列:\", greedy_decoded_text)\n",
    "print(\"Beam Search解码生成的文本序列:\", beam_decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
