{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb477331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d1bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力实现\n",
    "class GPT2SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, dropout=0.1):\n",
    "        super(GPT2SelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads == embedding_size), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(self.heads * self.head_dim, embedding_size)\n",
    "\n",
    "    def forward(self, values, keys, queries):\n",
    "        N = queries.shape[0]\n",
    "        values_len, keys_len, queries_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # 生成多头矩阵\n",
    "        values = values.view(N, values_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, keys_len, self.heads, self.head_dim)\n",
    "        queries = queries.view(N, queries_len, self.heads, self.head_dim)\n",
    "\n",
    "        Q = self.queries(queries)\n",
    "        K = self.keys(keys)\n",
    "        V = self.values(values)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K]) / (self.head_dim ** 0.5)  # Q和K的点积\n",
    "        # 应用单向掩码\n",
    "        mask = torch.tril(torch.ones(queries_len, keys_len)).expand(N, self.heads, queries_len, keys_len).to(energy.device)  # 下三角掩码\n",
    "        energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
    "        # 计算注意力权重\n",
    "        attn_weights = F.softmax(energy, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context = torch.einsum(\"nhqk,nkhd->nqhd\", [attn_weights, V]).reshape(N, queries_len, self.heads * self.head_dim)\n",
    "        out = self.fc_out(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa741536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_dim, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fefde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GPT-2模型基础块\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, embedding_size, heads, hidden_dim, dropout=0.1):\n",
    "        super(GPT2Block, self).__init__()\n",
    "        self.attention = GPT2SelfAttention(embedding_size, heads, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = FeedForward(embedding_size, hidden_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 自注意力机制\n",
    "        attn_output = self.attention(x, x, x)\n",
    "        norm1_out = self.layer_norm1(x + attn_output)\n",
    "        # 前馈神经网络\n",
    "        ffn_output = self.feed_forward(norm1_out)\n",
    "        norm2_out = self.layer_norm2(norm1_out + ffn_output)\n",
    "        return self.dropout(norm2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f784afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2模型的核心生成模块\n",
    "class GPT2TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_layers, heads, hidden_dim, max_len, dropout=0.1):\n",
    "        super(GPT2TextGenerator, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding = nn.Embedding(max_len, embedding_size)  # 假设最大序列长度为max_len\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GPT2Block(embedding_size, heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.size()\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(x.device)\n",
    "\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.fc_out(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, start_token, max_len, temperature=1.0):\n",
    "        generted = start_token\n",
    "        for _ in range(max_len - len(start_token)):\n",
    "            x = torch.tensor(generted).unsqueeze(0).to(next(self.parameters()).device)\n",
    "            logits = self.forward(x)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.argmax(probs, dim=-1).item()\n",
    "            generted.append(next_token)\n",
    "            if next_token == 0:  # 假设0是结束标记\n",
    "                break\n",
    "        return generted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1474e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "vocab_size = 10000\n",
    "embedding_size = 128\n",
    "num_layers = 4\n",
    "heads = 8\n",
    "hidden_dim = 512\n",
    "max_len = 50\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac7d5fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本序列（token ID）： [1, 5, 23, 67, 5862, 7848, 7616, 8325, 4800, 2124, 5675, 3472, 2857, 8936, 3392, 8376, 8529, 4373, 4797, 1435, 2181, 2999, 4079, 5864, 9716, 9418, 7600, 3505, 2545, 3668, 661, 5486, 4424, 3750, 7071, 7905, 8075, 861, 9934, 7897, 9760, 8688, 4282, 4917, 7785, 2173, 6474, 6709, 4608, 5641]\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "gpt2_model = GPT2TextGenerator(vocab_size, embedding_size, num_layers, heads, hidden_dim, max_len, dropout)\n",
    "# 模拟提示词输入\n",
    "start_token = [1, 5, 23, 67]  # 假设这些是提示词的token ID\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt2_model.to(device)\n",
    "start_token_tensor = torch.tensor(start_token).to(device)\n",
    "\n",
    "# 生成文本\n",
    "generated_sequence = gpt2_model.generate(start_token, max_len=max_len, temperature=1.0)\n",
    "print(\"生成的文本序列（token ID）：\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b6d281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本序列（解码后）： <token_1> <token_5> <token_23> <token_67> <token_5862> <token_7848> <token_7616> <token_8325> <token_4800> <token_2124> <token_5675> <token_3472> <token_2857> <token_8936> <token_3392> <token_8376> <token_8529> <token_4373> <token_4797> <token_1435> <token_2181> <token_2999> <token_4079> <token_5864> <token_9716> <token_9418> <token_7600> <token_3505> <token_2545> <token_3668> <token_661> <token_5486> <token_4424> <token_3750> <token_7071> <token_7905> <token_8075> <token_861> <token_9934> <token_7897> <token_9760> <token_8688> <token_4282> <token_4917> <token_7785> <token_2173> <token_6474> <token_6709> <token_4608> <token_5641>\n"
     ]
    }
   ],
   "source": [
    "# 模拟输出解码\n",
    "def decode_tokens(token_ids):\n",
    "    return \" \".join([f\"<token_{token_id}>\" for token_id in token_ids])\n",
    "\n",
    "decoded_text = decode_tokens(generated_sequence)\n",
    "print(\"生成的文本序列（解码后）：\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
