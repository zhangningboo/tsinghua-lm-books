{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a6e1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f07740bf690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from typing import List\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881b93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词器，将文本转换为词汇索引\n",
    "class Tokenizer:\n",
    "    def __init__(self, texts: List[str], min_freq: int=1, max_vocab_size: int=10000):\n",
    "        self.texts = texts\n",
    "        self.min_freq = min_freq\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab = self.build_vocab()\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # 删除所有不是范围内的所有字符\n",
    "        return text.split()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        word_counts = Counter()\n",
    "        for text in self.texts:\n",
    "            tokens = self.preprocess(text)\n",
    "            word_counts.update(tokens)\n",
    "        vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        for word, freq, in word_counts.most_common(self.max_vocab_size - 2):\n",
    "            if freq >= self.min_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def text_to_sequence(self, text: str):\n",
    "        tokens = self.preprocess(text)\n",
    "        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f90e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'the': 2,\n",
       " 'pytorch': 3,\n",
       " 'quick': 4,\n",
       " 'brown': 5,\n",
       " 'fox': 6,\n",
       " 'jumps': 7,\n",
       " 'over': 8,\n",
       " 'lazy': 9,\n",
       " 'dog': 10,\n",
       " 'is': 11,\n",
       " 'widely': 12,\n",
       " 'used': 13,\n",
       " 'for': 14,\n",
       " 'deep': 15,\n",
       " 'learning': 16,\n",
       " 'tasks': 17,\n",
       " 'natural': 18,\n",
       " 'language': 19,\n",
       " 'processing': 20,\n",
       " 'enables': 21,\n",
       " 'complex': 22,\n",
       " 'interactions': 23,\n",
       " 'this': 24,\n",
       " 'example': 25,\n",
       " 'demonstrates': 26,\n",
       " 'text': 27,\n",
       " 'embedding': 28,\n",
       " 'in': 29}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义示例文本数据\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"PyTorch is widely used for deep learning tasks\",\n",
    "    \"Natural language processing enables complex interactions\",\n",
    "    \"This example demonstrates text embedding in PyTorch\",\n",
    "]\n",
    "tokenizer = Tokenizer(texts)\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983e6a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 5, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义示例文本，并转换为索引序列\n",
    "text_sequence = tokenizer.text_to_sequence(\"The quick brown fox\")\n",
    "text_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be46fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入层定义，将词汇索引转换为嵌入向量\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f2bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBEDDING_DIM = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f47236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入层输出的形状： torch.Size([1, 4, 8])\n",
      "嵌入向量： tensor([[[ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,\n",
      "           1.6806],\n",
      "         [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057,\n",
      "          -0.7746],\n",
      "         [-1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347,\n",
      "          -0.4879],\n",
      "         [-0.9138, -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140,\n",
      "          -0.7360]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 嵌入层实例\n",
    "embedding_layer = TextEmbedding(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM).to(DEVICE)\n",
    "# 将索引序列转换为张量并传递到嵌入层\n",
    "text_tensor = torch.tensor([text_sequence], dtype=torch.long).to(DEVICE)\n",
    "embedded_output = embedding_layer(text_tensor)\n",
    "print(\"嵌入层输出的形状：\", embedded_output.shape)\n",
    "print(\"嵌入向量：\", embedded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0248bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入层训练示例\n",
    "optimizer = optim.Adam(embedding_layer.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e4cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设目标是另一个随机生成的嵌入向量\n",
    "target_embedding = torch.rand(embedded_output.shape).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6d251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后损失： 1.5542532205581665\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(embedded_output, target_embedding)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"训练后损失：\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a19780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练后损失0： 1.330237865447998\n",
      "训练后损失1： 1.311125636100769\n",
      "训练后损失2： 1.2922273874282837\n",
      "训练后损失3： 1.2735439538955688\n",
      "训练后损失4： 1.255075216293335\n",
      "训练后损失5： 1.2368210554122925\n",
      "训练后损失6： 1.21878182888031\n",
      "训练后损失7： 1.200957179069519\n",
      "训练后损失8： 1.183347463607788\n",
      "训练后损失9： 1.1659528017044067\n",
      "训练后损失10： 1.148773431777954\n",
      "训练后损失11： 1.1318092346191406\n",
      "训练后损失12： 1.1150603294372559\n",
      "训练后损失13： 1.098526120185852\n",
      "训练后损失14： 1.0822062492370605\n",
      "训练后损失15： 1.0660998821258545\n",
      "训练后损失16： 1.050205945968628\n",
      "训练后损失17： 1.034523367881775\n",
      "训练后损失18： 1.0190508365631104\n",
      "训练后损失19： 1.0037868022918701\n",
      "训练后损失20： 0.9887295961380005\n",
      "训练后损失21： 0.9738776087760925\n",
      "训练后损失22： 0.9592286348342896\n",
      "训练后损失23： 0.9447810649871826\n",
      "训练后损失24： 0.9305331707000732\n",
      "训练后损失25： 0.9164826273918152\n",
      "训练后损失26： 0.9026280641555786\n",
      "训练后损失27： 0.8889673948287964\n",
      "训练后损失28： 0.87549889087677\n",
      "训练后损失29： 0.8622208833694458\n",
      "训练后损失30： 0.8491311073303223\n",
      "训练后损失31： 0.8362277746200562\n",
      "训练后损失32： 0.8235086798667908\n",
      "训练后损失33： 0.8109720945358276\n",
      "训练后损失34： 0.7986156344413757\n",
      "训练后损失35： 0.7864373922348022\n",
      "训练后损失36： 0.7744353413581848\n",
      "训练后损失37： 0.7626074552536011\n",
      "训练后损失38： 0.7509515285491943\n",
      "训练后损失39： 0.7394654750823975\n",
      "训练后损失40： 0.728147029876709\n",
      "训练后损失41： 0.716994047164917\n",
      "训练后损失42： 0.7060043811798096\n",
      "训练后损失43： 0.69517582654953\n",
      "训练后损失44： 0.6845061779022217\n",
      "训练后损失45： 0.6739935874938965\n",
      "训练后损失46： 0.663635790348053\n",
      "训练后损失47： 0.653430700302124\n",
      "训练后损失48： 0.6433762311935425\n",
      "训练后损失49： 0.6334704160690308\n",
      "训练后损失50： 0.623711109161377\n",
      "训练后损失51： 0.6140962839126587\n",
      "训练后损失52： 0.6046237945556641\n",
      "训练后损失53： 0.595291793346405\n",
      "训练后损失54： 0.5860981941223145\n",
      "训练后损失55： 0.5770410895347595\n",
      "训练后损失56： 0.5681184530258179\n",
      "训练后损失57： 0.5593283772468567\n",
      "训练后损失58： 0.5506689548492432\n",
      "训练后损失59： 0.5421383380889893\n",
      "训练后损失60： 0.5337345600128174\n",
      "训练后损失61： 0.5254558324813843\n",
      "训练后损失62： 0.5173003673553467\n",
      "训练后损失63： 0.5092663764953613\n",
      "训练后损失64： 0.5013520121574402\n",
      "训练后损失65： 0.4935556650161743\n",
      "训练后损失66： 0.48587536811828613\n",
      "训练后损失67： 0.47830963134765625\n",
      "训练后损失68： 0.4708567261695862\n",
      "训练后损失69： 0.46351486444473267\n",
      "训练后损失70： 0.4562825560569763\n",
      "训练后损失71： 0.44915804266929626\n",
      "训练后损失72： 0.4421398341655731\n",
      "训练后损失73： 0.4352262616157532\n",
      "训练后损失74： 0.42841583490371704\n",
      "训练后损失75： 0.421706885099411\n",
      "训练后损失76： 0.41509801149368286\n",
      "训练后损失77： 0.4085876941680908\n",
      "训练后损失78： 0.40217453241348267\n",
      "训练后损失79： 0.39585691690444946\n",
      "训练后损失80： 0.3896335959434509\n",
      "训练后损失81： 0.3835029900074005\n",
      "训练后损失82： 0.37746384739875793\n",
      "训练后损失83： 0.3715147376060486\n",
      "训练后损失84： 0.365654319524765\n",
      "训练后损失85： 0.35988128185272217\n",
      "训练后损失86： 0.354194313287735\n",
      "训练后损失87： 0.3485921621322632\n",
      "训练后损失88： 0.3430735468864441\n",
      "训练后损失89： 0.3376372158527374\n",
      "训练后损失90： 0.3322819769382477\n",
      "训练后损失91： 0.32700657844543457\n",
      "训练后损失92： 0.32180991768836975\n",
      "训练后损失93： 0.31669074296951294\n",
      "训练后损失94： 0.3116479814052582\n",
      "训练后损失95： 0.30668050050735474\n",
      "训练后损失96： 0.3017871677875519\n",
      "训练后损失97： 0.29696691036224365\n",
      "训练后损失98： 0.2922186255455017\n",
      "训练后损失99： 0.28754130005836487\n"
     ]
    }
   ],
   "source": [
    "# 反向传播和优化\n",
    "for iter_cnt in range(100):\n",
    "    embedded_output = embedding_layer(text_tensor)\n",
    "    loss = criterion(embedded_output, target_embedding)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"训练后损失{iter_cnt}：\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f453714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嵌入层的权重矩阵：\n",
      " tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047],\n",
      "        [-0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "        [ 1.6323, -0.1496, -0.4874,  0.4296, -0.7481,  1.0683,  0.7908,  1.6706],\n",
      "        [ 1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599],\n",
      "        [-1.3747, -0.8612, -0.2134,  1.7074,  0.3289, -0.4145,  0.3157, -0.7646]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 显示嵌入层权重的部分\n",
    "print(\"嵌入层的权重矩阵：\\n\", embedding_layer.embedding.weight[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3-vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
