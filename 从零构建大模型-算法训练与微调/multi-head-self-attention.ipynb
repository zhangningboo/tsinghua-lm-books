{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac9be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x76d1b004ae70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e1f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_size // heads\n",
    "\n",
    "        assert(\n",
    "            self.head_dim * heads == embedding_size\n",
    "        ), \"Embedding Sieze 需要是heads的整数倍\"\n",
    "        # 线性变换用于生成Q、K、V矩阵\n",
    "        self.values = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
    "        self.queies = nn.Linear(in_features=self.head_dim, out_features=self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # 分头计算Q、K、V矩阵\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queies(queries)\n",
    "        # 计算Q与K的点积除以缩放因子\n",
    "        energy = torch.einsum(\n",
    "            \"nqhd,nkhd->nhqk\", [queries, keys]\n",
    "        ) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # 计算注意力权重\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        # 注意力权重乘以V\n",
    "        out = torch.einsum(\n",
    "            \"nhql,nlhd->nqhd\", [attention, values]\n",
    "        ).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        return self.fc_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdab11d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "embedding_size = 128\n",
    "heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b6f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建随机输入\n",
    "values = torch.rand(batch_size, seq_length, embedding_size)\n",
    "keys = torch.rand(batch_size, seq_length, embedding_size)\n",
    "queries = torch.rand(batch_size, seq_length, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1ff2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化自注意力层\n",
    "self_attention_layer = MultiHeadSelfAttention(embedding_size, heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e730ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出的形状： torch.Size([2, 10, 128])\n",
      "自注意力的输出： tensor([[[-0.0785, -0.1186, -0.0190,  ..., -0.0991,  0.1303, -0.2674],\n",
      "         [-0.0786, -0.1176, -0.0183,  ..., -0.0985,  0.1302, -0.2675],\n",
      "         [-0.0783, -0.1177, -0.0180,  ..., -0.0991,  0.1306, -0.2683],\n",
      "         ...,\n",
      "         [-0.0778, -0.1172, -0.0187,  ..., -0.0975,  0.1294, -0.2678],\n",
      "         [-0.0771, -0.1180, -0.0183,  ..., -0.0981,  0.1296, -0.2677],\n",
      "         [-0.0776, -0.1186, -0.0188,  ..., -0.0989,  0.1297, -0.2675]],\n",
      "\n",
      "        [[-0.1401, -0.1019,  0.0584,  ..., -0.0881,  0.1732, -0.2754],\n",
      "         [-0.1425, -0.1028,  0.0614,  ..., -0.0898,  0.1736, -0.2751],\n",
      "         [-0.1413, -0.1036,  0.0605,  ..., -0.0869,  0.1745, -0.2743],\n",
      "         ...,\n",
      "         [-0.1406, -0.1026,  0.0611,  ..., -0.0874,  0.1745, -0.2758],\n",
      "         [-0.1405, -0.1027,  0.0588,  ..., -0.0880,  0.1739, -0.2755],\n",
      "         [-0.1410, -0.1030,  0.0598,  ..., -0.0869,  0.1757, -0.2748]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output = self_attention_layer(values, keys, queries, mask=None)\n",
    "print(\"输出的形状：\", output.shape)  # 应该输出 torch.Size([2, 10, 128])\n",
    "print(\"自注意力的输出：\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力权重计算\n",
    "class MultiHeadSelfAttentionWithWeights(MultiHeadSelfAttention):\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # 分头计算Q、K、V矩阵\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queies(queries)\n",
    "        # 计算Q与K的点积除以缩放因子\n",
    "        energy = torch.einsum(\n",
    "            \"nqhd,nkhd->nhqk\", [queries, keys]\n",
    "        ) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        # 计算注意力权重\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "        # 注意力权重乘以V\n",
    "        out = torch.einsum(\n",
    "            \"nhql,nlhd->nqhd\", [attention, values]\n",
    "        ).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        return self.fc_out(out), attention  # 以上代码同SelfAttention类，只不过多返回注意力权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e23c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出的形状： torch.Size([2, 10, 128])\n",
      "自注意力的输出： tensor([[[ 0.1922, -0.1950,  0.1733,  ...,  0.2255,  0.1212, -0.0773],\n",
      "         [ 0.1943, -0.1952,  0.1743,  ...,  0.2263,  0.1215, -0.0771],\n",
      "         [ 0.1932, -0.1947,  0.1725,  ...,  0.2266,  0.1221, -0.0769],\n",
      "         ...,\n",
      "         [ 0.1928, -0.1929,  0.1728,  ...,  0.2259,  0.1231, -0.0769],\n",
      "         [ 0.1951, -0.1950,  0.1735,  ...,  0.2264,  0.1248, -0.0794],\n",
      "         [ 0.1941, -0.1933,  0.1739,  ...,  0.2258,  0.1211, -0.0783]],\n",
      "\n",
      "        [[ 0.1323, -0.0954,  0.2114,  ...,  0.2396,  0.0258, -0.0900],\n",
      "         [ 0.1307, -0.0982,  0.2128,  ...,  0.2388,  0.0257, -0.0892],\n",
      "         [ 0.1314, -0.0977,  0.2126,  ...,  0.2375,  0.0265, -0.0898],\n",
      "         ...,\n",
      "         [ 0.1310, -0.0961,  0.2101,  ...,  0.2384,  0.0250, -0.0900],\n",
      "         [ 0.1319, -0.0962,  0.2115,  ...,  0.2381,  0.0262, -0.0896],\n",
      "         [ 0.1315, -0.0954,  0.2109,  ...,  0.2403,  0.0252, -0.0905]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "注意力权重的形状： torch.Size([2, 8, 10, 10])\n",
      "注意力权重： tensor([[[[0.0993, 0.0992, 0.0975,  ..., 0.0997, 0.1012, 0.1038],\n",
      "          [0.1026, 0.0981, 0.0929,  ..., 0.0932, 0.1065, 0.1104],\n",
      "          [0.1005, 0.0988, 0.0946,  ..., 0.0986, 0.1007, 0.1056],\n",
      "          ...,\n",
      "          [0.0972, 0.1009, 0.0986,  ..., 0.0989, 0.1002, 0.1044],\n",
      "          [0.1029, 0.1002, 0.0996,  ..., 0.0942, 0.1055, 0.1044],\n",
      "          [0.1014, 0.0982, 0.0950,  ..., 0.0973, 0.1040, 0.1061]],\n",
      "\n",
      "         [[0.1019, 0.1032, 0.0978,  ..., 0.0925, 0.1031, 0.0920],\n",
      "          [0.1026, 0.1022, 0.0982,  ..., 0.0957, 0.1031, 0.0940],\n",
      "          [0.0989, 0.0979, 0.1010,  ..., 0.0973, 0.1005, 0.1000],\n",
      "          ...,\n",
      "          [0.0955, 0.0990, 0.1033,  ..., 0.0979, 0.1002, 0.0997],\n",
      "          [0.0983, 0.1016, 0.1020,  ..., 0.0927, 0.1035, 0.0931],\n",
      "          [0.1035, 0.1047, 0.0983,  ..., 0.0900, 0.1037, 0.0899]],\n",
      "\n",
      "         [[0.1043, 0.0933, 0.0891,  ..., 0.1136, 0.0895, 0.1032],\n",
      "          [0.1036, 0.0976, 0.0888,  ..., 0.1051, 0.0940, 0.1004],\n",
      "          [0.1053, 0.0952, 0.0896,  ..., 0.1064, 0.0933, 0.1011],\n",
      "          ...,\n",
      "          [0.1058, 0.0957, 0.0853,  ..., 0.1140, 0.0867, 0.1018],\n",
      "          [0.0954, 0.0983, 0.0982,  ..., 0.1030, 0.1030, 0.0989],\n",
      "          [0.0974, 0.0960, 0.0954,  ..., 0.1059, 0.1009, 0.0992]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0997, 0.1023, 0.1016,  ..., 0.0948, 0.0938, 0.0884],\n",
      "          [0.0979, 0.1018, 0.0995,  ..., 0.0964, 0.0993, 0.0953],\n",
      "          [0.0997, 0.0943, 0.1059,  ..., 0.0969, 0.0951, 0.0939],\n",
      "          ...,\n",
      "          [0.0962, 0.0964, 0.1040,  ..., 0.0921, 0.0908, 0.0959],\n",
      "          [0.0976, 0.1021, 0.0973,  ..., 0.1009, 0.0943, 0.0931],\n",
      "          [0.0961, 0.0988, 0.1008,  ..., 0.0995, 0.0925, 0.0983]],\n",
      "\n",
      "         [[0.1027, 0.0977, 0.1029,  ..., 0.1110, 0.1043, 0.1050],\n",
      "          [0.1055, 0.0977, 0.1011,  ..., 0.1117, 0.1026, 0.1055],\n",
      "          [0.1045, 0.0928, 0.1014,  ..., 0.1117, 0.1048, 0.1042],\n",
      "          ...,\n",
      "          [0.1005, 0.0971, 0.1028,  ..., 0.1082, 0.1055, 0.0990],\n",
      "          [0.1008, 0.0947, 0.1019,  ..., 0.1101, 0.1042, 0.0993],\n",
      "          [0.0976, 0.0958, 0.1038,  ..., 0.1091, 0.1045, 0.1022]],\n",
      "\n",
      "         [[0.1087, 0.0963, 0.1032,  ..., 0.1030, 0.0968, 0.0981],\n",
      "          [0.1069, 0.0995, 0.1016,  ..., 0.1056, 0.0984, 0.0984],\n",
      "          [0.1066, 0.0987, 0.1024,  ..., 0.1090, 0.0919, 0.0950],\n",
      "          ...,\n",
      "          [0.1059, 0.1004, 0.1091,  ..., 0.0973, 0.0970, 0.1004],\n",
      "          [0.1068, 0.0955, 0.0973,  ..., 0.1068, 0.0956, 0.0998],\n",
      "          [0.1101, 0.0981, 0.1093,  ..., 0.1024, 0.0959, 0.0996]]],\n",
      "\n",
      "\n",
      "        [[[0.1087, 0.1014, 0.1006,  ..., 0.1002, 0.0929, 0.0955],\n",
      "          [0.1129, 0.1049, 0.1012,  ..., 0.1032, 0.0914, 0.0947],\n",
      "          [0.1088, 0.1071, 0.1045,  ..., 0.0983, 0.0965, 0.1003],\n",
      "          ...,\n",
      "          [0.1076, 0.1029, 0.1055,  ..., 0.1028, 0.0927, 0.0985],\n",
      "          [0.1019, 0.1052, 0.1028,  ..., 0.1002, 0.0983, 0.1024],\n",
      "          [0.1111, 0.1010, 0.0966,  ..., 0.1011, 0.0932, 0.0934]],\n",
      "\n",
      "         [[0.1017, 0.1004, 0.0948,  ..., 0.0949, 0.0997, 0.0972],\n",
      "          [0.1057, 0.0976, 0.0931,  ..., 0.0928, 0.0940, 0.1051],\n",
      "          [0.1013, 0.0998, 0.0948,  ..., 0.0935, 0.0970, 0.0996],\n",
      "          ...,\n",
      "          [0.1051, 0.0980, 0.0944,  ..., 0.0939, 0.0936, 0.1050],\n",
      "          [0.1023, 0.0993, 0.0971,  ..., 0.0921, 0.0996, 0.0939],\n",
      "          [0.1060, 0.0963, 0.0986,  ..., 0.0933, 0.0921, 0.1033]],\n",
      "\n",
      "         [[0.0983, 0.0998, 0.0906,  ..., 0.0919, 0.0970, 0.1112],\n",
      "          [0.1053, 0.1000, 0.0899,  ..., 0.0925, 0.0989, 0.1052],\n",
      "          [0.1001, 0.1022, 0.0970,  ..., 0.0928, 0.0991, 0.1028],\n",
      "          ...,\n",
      "          [0.0973, 0.0983, 0.0951,  ..., 0.0950, 0.0968, 0.1083],\n",
      "          [0.0987, 0.0998, 0.0893,  ..., 0.0930, 0.0973, 0.1106],\n",
      "          [0.0974, 0.1029, 0.0936,  ..., 0.0971, 0.1008, 0.1045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1133, 0.1047, 0.0943,  ..., 0.1003, 0.1020, 0.0994],\n",
      "          [0.1181, 0.1050, 0.0934,  ..., 0.0955, 0.1017, 0.0990],\n",
      "          [0.1188, 0.1036, 0.0947,  ..., 0.0950, 0.1004, 0.1019],\n",
      "          ...,\n",
      "          [0.1144, 0.1005, 0.0949,  ..., 0.0990, 0.1039, 0.1027],\n",
      "          [0.1128, 0.0980, 0.0989,  ..., 0.0987, 0.0996, 0.1013],\n",
      "          [0.1128, 0.1102, 0.0918,  ..., 0.0978, 0.1000, 0.0984]],\n",
      "\n",
      "         [[0.0986, 0.0915, 0.1037,  ..., 0.0929, 0.1047, 0.0923],\n",
      "          [0.0936, 0.0967, 0.1024,  ..., 0.0913, 0.1109, 0.0958],\n",
      "          [0.0922, 0.0960, 0.1042,  ..., 0.0924, 0.1100, 0.0966],\n",
      "          ...,\n",
      "          [0.0892, 0.1045, 0.0980,  ..., 0.0974, 0.1134, 0.1041],\n",
      "          [0.0938, 0.0970, 0.1018,  ..., 0.0944, 0.1073, 0.0990],\n",
      "          [0.0939, 0.0954, 0.1021,  ..., 0.0920, 0.1098, 0.0967]],\n",
      "\n",
      "         [[0.1004, 0.1057, 0.0995,  ..., 0.1028, 0.0977, 0.0948],\n",
      "          [0.0997, 0.1058, 0.0960,  ..., 0.1016, 0.0988, 0.0980],\n",
      "          [0.0974, 0.1114, 0.0931,  ..., 0.1027, 0.0980, 0.0983],\n",
      "          ...,\n",
      "          [0.1025, 0.1068, 0.0958,  ..., 0.1000, 0.0996, 0.0972],\n",
      "          [0.1016, 0.1082, 0.0960,  ..., 0.1024, 0.0968, 0.0939],\n",
      "          [0.1002, 0.1056, 0.0984,  ..., 0.1026, 0.0931, 0.0971]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 初始化自注意力层\n",
    "self_attention_layer = MultiHeadSelfAttentionWithWeights(embedding_size, heads)\n",
    "# 前向传播\n",
    "output, attention_weights = self_attention_layer(values, keys, queries, mask=None)\n",
    "print(\"输出的形状：\", output.shape)  # 应该输出 torch.Size([2, 10, 128])，与输入保持相同，利于层堆叠\n",
    "print(\"自注意力的输出：\", output)\n",
    "print(\"注意力权重的形状：\", attention_weights.shape)  # 应该输出 torch.Size([2, 8, 10, 10])\n",
    "print(\"注意力权重：\", attention_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frome_zero_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
