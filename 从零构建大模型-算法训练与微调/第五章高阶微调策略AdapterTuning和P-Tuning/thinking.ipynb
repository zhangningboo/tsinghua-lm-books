{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c2f89d",
   "metadata": {},
   "source": [
    "# 2、描述LoRA Tuning的低秩分解原理，并在代码层面说明如何使用nn.Linear模块来创建低秩分解的降维和升维矩阵。如何在前向传播中将LoRA模块嵌入BERT的注意力层中？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcc518",
   "metadata": {},
   "source": [
    "下面从**原理 → 模块化实现 → 与 BERT 注意力层的集成**三个层次，系统说明 **LoRA (Low-Rank Adaptation)** 的低秩分解思想及其在代码层面的落地方式。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、LoRA Tuning 的低秩分解原理\n",
    "\n",
    "### 1. 标准微调的问题\n",
    "\n",
    "以 Transformer / BERT 中的线性层为例（如 Attention 的 $W_q, W_k, W_v, W_o$）：\n",
    "\n",
    "$\n",
    "y = x W,\\quad W \\in \\mathbb{R}^{d_{in} \\times d_{out}}\n",
    "$\n",
    "\n",
    "全量微调需要对 ($W$) 的全部参数更新，参数量与显存开销巨大。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. LoRA 的核心思想\n",
    "\n",
    "LoRA **冻结原始权重 (W_0)**，仅学习一个**低秩增量矩阵**：\n",
    "\n",
    "$\n",
    "W = W_0 + \\Delta W\n",
    "$\n",
    "\n",
    "并对增量进行低秩分解：\n",
    "\n",
    "$\n",
    "\\Delta W = B A\n",
    "$\n",
    "\n",
    "其中：\n",
    "\n",
    "* ($A \\in \\mathbb{R}^{r \\times d_{in}}$)（降维）\n",
    "* ($B \\in \\mathbb{R}^{d_{out} \\times r}$)（升维）\n",
    "* ($r \\ll \\min(d_{in}, d_{out})$)\n",
    "\n",
    "因此前向传播变为：\n",
    "\n",
    "$\n",
    "y = x W_0 + x A^T B^T\n",
    "$\n",
    "\n",
    "通常再引入缩放系数：\n",
    "\n",
    "$\n",
    "y = x W_0 + \\alpha / r \\cdot x A^T B^T\n",
    "$\n",
    "\n",
    "> **关键假设**：下游任务所需的权重更新位于一个低维子空间中。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 参数量对比\n",
    "\n",
    "* 全量微调：($d_{in} \\times d_{out}$)\n",
    "* LoRA：($r(d_{in} + d_{out})$)\n",
    "\n",
    "当 $r=8, 16$ 时，参数量下降一个数量级以上。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、用 `nn.Linear` 实现 LoRA 的低秩分解\n",
    "\n",
    "### 1. LoRA 线性模块结构\n",
    "\n",
    "LoRA 不替换原线性层，而是**并联一个低秩分支**：\n",
    "\n",
    "```text\n",
    "x ── Linear(W0, frozen) ──┐\n",
    "                          ├─ add ─→ y\n",
    "x ── Linear(A) → Linear(B) ─┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. LoRA Linear 的最小实现\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 原始线性层（冻结）\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.linear.weight.requires_grad = False\n",
    "        if bias:\n",
    "            self.linear.bias.requires_grad = False\n",
    "\n",
    "        # LoRA 低秩分解\n",
    "        self.lora_A = nn.Linear(in_features, r, bias=False)   # 降维\n",
    "        self.lora_B = nn.Linear(r, out_features, bias=False)  # 升维\n",
    "\n",
    "        # 缩放因子\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "        # 初始化：A 正态，B 全 0（保证初始等价于原模型）\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.scaling * self.lora_B(self.lora_A(x))\n",
    "```\n",
    "\n",
    "### 3. 关键设计点说明\n",
    "\n",
    "* **B 初始化为 0**\n",
    "  → 初始阶段 $ΔW = 0$，模型行为不变\n",
    "* **只训练 A / B**\n",
    "\n",
    "  ```python\n",
    "  for n, p in model.named_parameters():\n",
    "      if \"lora\" not in n:\n",
    "          p.requires_grad = False\n",
    "  ```\n",
    "* **LoRA 是加性结构，不破坏原模型**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、LoRA 嵌入 BERT 注意力层的方式\n",
    "\n",
    "### 1. BERT Self-Attention 中的关键线性层\n",
    "\n",
    "在 `BertSelfAttention` 中：\n",
    "\n",
    "```python\n",
    "self.query = nn.Linear(hidden_size, hidden_size)\n",
    "self.key   = nn.Linear(hidden_size, hidden_size)\n",
    "self.value = nn.Linear(hidden_size, hidden_size)\n",
    "```\n",
    "\n",
    "LoRA 通常作用在：\n",
    "\n",
    "* `query`\n",
    "* `value`\n",
    "  （实践中对 `key` 效果收益较小）\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 替换 Attention 线性层为 LoRA 版本\n",
    "\n",
    "#### 方法 1：直接替换模块（侵入式）\n",
    "\n",
    "```python\n",
    "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
    "\n",
    "def inject_lora_into_bert_attention(attn, r=8, alpha=32):\n",
    "    attn.query = LoRALinear(\n",
    "        attn.query.in_features,\n",
    "        attn.query.out_features,\n",
    "        r=r,\n",
    "        lora_alpha=alpha\n",
    "    )\n",
    "    attn.value = LoRALinear(\n",
    "        attn.value.in_features,\n",
    "        attn.value.out_features,\n",
    "        r=r,\n",
    "        lora_alpha=alpha\n",
    "    )\n",
    "```\n",
    "\n",
    "应用到所有 encoder layer：\n",
    "\n",
    "```python\n",
    "for layer in model.bert.encoder.layer:\n",
    "    inject_lora_into_bert_attention(layer.attention.self)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 前向传播中 LoRA 的位置（数学视角）\n",
    "\n",
    "以 `query` 为例：\n",
    "\n",
    "$\n",
    "Q = X (W_q + B_q A_q) = X W_q + X A_q B_q\n",
    "$\n",
    "\n",
    "然后进入标准 Attention 计算：\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V)\n",
    "$\n",
    "\n",
    "LoRA **完全不改动** Attention 的 softmax / mask / head 拆分逻辑。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 前向传播路径（代码层面）\n",
    "\n",
    "```python\n",
    "def forward(self, hidden_states):\n",
    "    query_layer = self.query(hidden_states)  # 已含 LoRA 增量\n",
    "    key_layer   = self.key(hidden_states)\n",
    "    value_layer = self.value(hidden_states)\n",
    "\n",
    "    # 后续与原 BERT 完全一致\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 四、训练与推理阶段的注意事项\n",
    "\n",
    "### 1. 训练阶段\n",
    "\n",
    "* 冻结 backbone\n",
    "* 只优化 LoRA 参数\n",
    "* 可使用较大学习率（1e-3 ~ 1e-4）\n",
    "\n",
    "### 2. 推理阶段（可选合并）\n",
    "\n",
    "LoRA 可**合并回原权重**：\n",
    "\n",
    "$\n",
    "W' = W_0 + BA\n",
    "$\n",
    "\n",
    "适合部署场景，避免额外计算图。\n",
    "\n",
    "---\n",
    "\n",
    "**LoRA / Adapter / Prefix-Tuning / P-Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "## 一、核心机制对比（本质差异）\n",
    "\n",
    "| 方法                | 注入空间           | 核心思想                      |\n",
    "| ----------------- | -------------- | ------------------------- |\n",
    "| **LoRA**          | **权重空间**       | 用低秩矩阵近似权重更新               |\n",
    "| **Adapter**       | 网络结构空间         | 插入瓶颈子网络进行特征重映射            |\n",
    "| **Prefix-Tuning** | 注意力表示空间        | 给每层 Attention 提供可学习 KV 前缀 |\n",
    "| **P-Tuning**      | 表示 / Prompt 空间 | 用连续可训练 Prompt 引导表示        |\n",
    "\n",
    "**一句话区分**\n",
    "\n",
    "* **LoRA**：改“怎么线性映射”\n",
    "* **Adapter**：加“新的非线性变换路径”\n",
    "* **Prefix**：改“注意力看到什么”\n",
    "* **P-Tuning**：改“模型从什么语境开始理解任务”\n",
    "\n",
    "---\n",
    "\n",
    "## 二、参数化形式（数学视角）\n",
    "\n",
    "### 1️⃣ LoRA（低秩权重更新）\n",
    "\n",
    "$\n",
    "W = W_0 + \\Delta W,\\quad \\Delta W = BA\n",
    "$\n",
    "\n",
    "* ($A \\in \\mathbb{R}^{r \\times d_{in}}$)\n",
    "* ($B \\in \\mathbb{R}^{d_{out} \\times r}$)\n",
    "\n",
    "👉 更新发生在 **Linear 层**\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Adapter（瓶颈残差）\n",
    "\n",
    "$\n",
    "h' = h + W_{up},\\sigma(W_{down}h)\n",
    "$\n",
    "\n",
    "* 典型 r ≪ d\n",
    "* 插入在 Transformer 层中\n",
    "\n",
    "👉 更新发生在 **层间残差路径**\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Prefix-Tuning（KV 前缀）\n",
    "\n",
    "$\n",
    "\\text{Attn}(Q, [K_p;K], [V_p;V])\n",
    "$\n",
    "\n",
    "* Prefix 是连续向量\n",
    "* 每层、每头可独立\n",
    "\n",
    "👉 更新发生在 **Attention 内部**\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ P-Tuning（连续 Prompt）\n",
    "\n",
    "$\n",
    "[x_1,\\dots,x_n] \\Rightarrow [p_1,\\dots,p_k,x_1,\\dots,x_n]\n",
    "$\n",
    "\n",
    "* Prompt 可参数化（MLP / LSTM）\n",
    "* v2 中 Prompt 可“下沉到每层”\n",
    "\n",
    "👉 更新发生在 **Embedding / Hidden 表示层**\n",
    "\n",
    "---\n",
    "\n",
    "## 三、注入位置对比（结构视角）\n",
    "\n",
    "```text\n",
    "Input ──► Embedding ──► Transformer Blocks ──► Output\n",
    "          ▲                ▲        ▲\n",
    "        P-Tuning        Prefix     Adapter\n",
    "                             ▲\n",
    "                           LoRA\n",
    "```\n",
    "\n",
    "| 方法       | Embedding | Attention | FFN | Linear |\n",
    "| -------- | --------- | --------- | --- | ------ |\n",
    "| LoRA     | ❌         | ❌         | ❌   | ✅      |\n",
    "| Adapter  | ❌         | ⚠️        | ✅   | ❌      |\n",
    "| Prefix   | ❌         | ✅         | ❌   | ❌      |\n",
    "| P-Tuning | ✅         | ⚠️        | ❌   | ❌      |\n",
    "\n",
    "---\n",
    "\n",
    "## 四、参数效率 & 推理代价\n",
    "\n",
    "| 方法       | 训练参数量 | 推理额外计算 | 可合并权重 |\n",
    "| -------- | ----- | ------ | ----- |\n",
    "| **LoRA** | ⭐⭐⭐   | ⭐      | ✅     |\n",
    "| Adapter  | ⭐⭐    | ⭐⭐     | ❌     |\n",
    "| Prefix   | ⭐⭐    | ⭐⭐⭐    | ❌     |\n",
    "| P-Tuning | ⭐⭐⭐⭐  | ⭐      | ❌     |\n",
    "\n",
    "> ⭐ 越多表示越优 / 越高\n",
    "\n",
    "---\n",
    "\n",
    "## 五、效果与稳定性（经验结论）\n",
    "\n",
    "### 🔹 小模型（BERT / RoBERTa-base）\n",
    "\n",
    "> **LoRA ≥ Adapter > Prefix > P-Tuning**\n",
    "\n",
    "原因：\n",
    "\n",
    "* 小模型容量有限\n",
    "* 表示空间 Prompt 表达力不足\n",
    "* 权重空间调整更直接\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 大模型（LLM / >10B）\n",
    "\n",
    "> **LoRA ≈ Prefix ≥ P-Tuning > Adapter**\n",
    "\n",
    "原因：\n",
    "\n",
    "* 大模型对 Prompt 敏感\n",
    "* Prefix 能显式控制 Attention\n",
    "* Adapter 带来额外推理成本\n",
    "\n",
    "---\n",
    "\n",
    "## 六、工程与研究选型建议\n",
    "\n",
    "### ✅ 如果你在做 **BERT / Encoder 模型实验**\n",
    "\n",
    "* **首选：LoRA**\n",
    "* 对照方法：Adapter、Prefix\n",
    "* P-Tuning 作为轻量 baseline\n",
    "\n",
    "### ✅ 如果你关注 **多任务 / 多 Adapter 切换**\n",
    "\n",
    "* Adapter（任务隔离好）\n",
    "* Prefix（KV 可缓存）\n",
    "\n",
    "### ✅ 如果你做 **部署 / 合并权重**\n",
    "\n",
    "* **只有 LoRA 能“无侵入合并”**\n",
    "\n",
    "---\n",
    "\n",
    "## 七、与“可解释性 / 控制性”的关系\n",
    "\n",
    "| 方法       | 可解释性                 |\n",
    "| -------- | -------------------- |\n",
    "| Prefix   | ⭐⭐⭐⭐（显式控制 Attention） |\n",
    "| P-Tuning | ⭐⭐⭐                  |\n",
    "| Adapter  | ⭐⭐                   |\n",
    "| LoRA     | ⭐（权重空间隐式）            |\n",
    "\n",
    "---\n",
    "\n",
    "## 八、总结性对照（一句话版）\n",
    "\n",
    "* **LoRA**：最工程化、最稳健、最通用\n",
    "* **Adapter**：结构增强，适合小模型\n",
    "* **Prefix**：Attention 控制型 PEFT\n",
    "* **P-Tuning**：最轻量，依赖模型规模\n",
    "\n",
    "> **从“影响路径深度”排序：**\n",
    "> Adapter ≈ LoRA > Prefix > P-Tuning\n",
    ">\n",
    "> **从“参数效率”排序：**\n",
    "> P-Tuning > LoRA > Prefix > Adapter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493c038",
   "metadata": {},
   "source": [
    "# 11、LoRA Tuning的降维和升维矩阵的设置如何影响微调效果？请说明LoRA Tuning的实现中，如何选择适当的秩值，以达到平衡参数量和性能的效果，并描述在代码中如何设置这些矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae0d99",
   "metadata": {},
   "source": [
    "下面从**机理 → 超参数影响 → 经验选型 → 代码实现**四个层面，系统回答你这个问题，角度偏**研究 + 工程可落地**。\n",
    "\n",
    "---\n",
    "\n",
    "## 一、LoRA 中降维 / 升维矩阵的机理作用\n",
    "\n",
    "在 LoRA 中，原始线性映射被改写为：\n",
    "\n",
    "$\n",
    "y = xW_0 + \\alpha / r \\cdot xA^\\top B^\\top\n",
    "$\n",
    "\n",
    "其中：\n",
    "\n",
    "* **降维矩阵 (A)**：\n",
    "  $\n",
    "  A: \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{r}\n",
    "  $\n",
    "  负责**提取“可更新子空间”**\n",
    "\n",
    "* **升维矩阵 (B)**：\n",
    "  $\n",
    "  B: \\mathbb{R}^{r} \\rightarrow \\mathbb{R}^{d_{out}}\n",
    "  $\n",
    "  负责**将低维更新映射回原空间**\n",
    "\n",
    "👉 本质上：\n",
    "\n",
    "> **LoRA 将权重更新限制在一个 r 维子空间中**\n",
    "\n",
    "---\n",
    "\n",
    "## 二、秩值 r 对微调效果的影响（核心）\n",
    "\n",
    "### 1️⃣ r 是 LoRA 的“模型容量旋钮”\n",
    "\n",
    "| r           | 含义         |\n",
    "| ----------- | ---------- |\n",
    "| r = 0       | 不进行微调      |\n",
    "| r 很小（1–4）   | 强正则，可能欠拟合  |\n",
    "| r 适中（8–16）  | 性能 / 参数平衡点 |\n",
    "| r 很大（32–64） | 接近全量微调     |\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ r 对模型的影响维度\n",
    "\n",
    "#### （1）表达能力\n",
    "\n",
    "$\n",
    "\\text{rank}(\\Delta W) \\le r\n",
    "$\n",
    "\n",
    "* r 越大，可学习的方向越多\n",
    "* r 太小 → 更新受限 → 学不到复杂任务差异\n",
    "\n",
    "#### （2）参数量\n",
    "\n",
    "$\n",
    "params = r(d_{in} + d_{out})\n",
    "$\n",
    "\n",
    "以 BERT attention 为例（768×768）：\n",
    "\n",
    "| r  | 参数量 |\n",
    "| -- | --- |\n",
    "| 4  | 6k  |\n",
    "| 8  | 12k |\n",
    "| 16 | 24k |\n",
    "| 64 | 98k |\n",
    "\n",
    "> 对比全量：768² ≈ 590k\n",
    "\n",
    "---\n",
    "\n",
    "#### （3）优化稳定性\n",
    "\n",
    "* r 小：\n",
    "\n",
    "  * 梯度集中，训练稳定\n",
    "* r 大：\n",
    "\n",
    "  * 更灵活，但容易过拟合\n",
    "  * 对学习率更敏感\n",
    "\n",
    "---\n",
    "\n",
    "## 三、如何选择合适的秩 r（实战经验）\n",
    "\n",
    "### 1️⃣ 经验法则（强烈推荐）\n",
    "\n",
    "| 场景                     | 推荐 r       |\n",
    "| ---------------------- | ---------- |\n",
    "| BERT-base / Encoder 模型 | **8 或 16** |\n",
    "| 小数据集 / 强正则需求           | 4 – 8      |\n",
    "| 大模型 / 高复杂任务            | 16 – 32    |\n",
    "| 接近全量微调                 | ≥ 64       |\n",
    "\n",
    "> **80% 的实验，r=8 已经很接近全量微调效果**\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 层级差异化（进阶）\n",
    "\n",
    "不是所有层都需要同样的 r：\n",
    "\n",
    "| 层  | 建议         |\n",
    "| -- | ---------- |\n",
    "| 低层 | r 小（4–8）   |\n",
    "| 高层 | r 大（16–32） |\n",
    "\n",
    "这种 **Layer-wise r** 在表示迁移任务中特别有效。\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 与缩放因子 α 的关系（经常被忽略）\n",
    "\n",
    "LoRA 实际更新是：\n",
    "\n",
    "$\n",
    "\\Delta W = \\frac{\\alpha}{r} BA\n",
    "$\n",
    "\n",
    "经验建议：\n",
    "\n",
    "$\n",
    "\\alpha \\approx 2r \\sim 4r\n",
    "$\n",
    "\n",
    "常见组合：\n",
    "\n",
    "| r  | α     |\n",
    "| -- | ----- |\n",
    "| 4  | 8–16  |\n",
    "| 8  | 16–32 |\n",
    "| 16 | 32–64 |\n",
    "\n",
    "👉 **α/r 决定更新幅度，不是 α 本身**\n",
    "\n",
    "---\n",
    "\n",
    "## 四、在代码中如何设置降维 / 升维矩阵\n",
    "\n",
    "### 1️⃣ 标准 LoRA Linear 实现（推荐写法）\n",
    "\n",
    "```python\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_linear: nn.Linear,\n",
    "        r: int = 8,\n",
    "        lora_alpha: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        self.base.weight.requires_grad = False\n",
    "\n",
    "        in_dim = base_linear.in_features\n",
    "        out_dim = base_linear.out_features\n",
    "\n",
    "        # 降维矩阵 A: d_in → r\n",
    "        self.lora_A = nn.Linear(in_dim, r, bias=False)\n",
    "\n",
    "        # 升维矩阵 B: r → d_out\n",
    "        self.lora_B = nn.Linear(r, out_dim, bias=False)\n",
    "\n",
    "        self.scaling = lora_alpha / r\n",
    "\n",
    "        # 关键初始化\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x) + self.scaling * self.lora_B(self.lora_A(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ 为什么 B 初始化为 0？\n",
    "\n",
    "* 保证初始阶段：\n",
    "  $\n",
    "  \\Delta W = 0\n",
    "  $\n",
    "* 模型行为 ≡ 原始预训练模型\n",
    "* **训练更稳定，loss 不会一开始就炸**\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 如何验证 r 是否足够？\n",
    "\n",
    "你可以做一个**极小代价消融实验**：\n",
    "\n",
    "```text\n",
    "r = 2, 4, 8, 16\n",
    "↓\n",
    "画 val acc / loss\n",
    "↓\n",
    "找到拐点\n",
    "```\n",
    "\n",
    "通常在 r=8 或 r=16 出现**收益递减点**。\n",
    "\n",
    "---\n",
    "\n",
    "## 五、常见误区（重要）\n",
    "\n",
    "### ❌ r 越大越好\n",
    "\n",
    "→ 错，r 是**容量约束 + 正则项**\n",
    "\n",
    "### ❌ α 不重要\n",
    "\n",
    "→ 错，α/r 才是真正起作用的量\n",
    "\n",
    "### ❌ 所有 Linear 都加 LoRA\n",
    "\n",
    "→ 通常只对 **Q / V** 效果最好\n",
    "\n",
    "---\n",
    "\n",
    "## 六、总结性回答（可直接写进论文 / 报告）\n",
    "\n",
    "> LoRA 通过在权重空间中引入低秩增量，将模型更新限制在 r 维子空间内。秩值 r 控制了可学习更新的自由度，是平衡参数效率与性能的关键超参数。较小的 r 提供更强的正则化，而适中的 r（如 8 或 16）在多数任务中即可接近全量微调性能。工程实现中，LoRA 通过一个降维矩阵和一个升维矩阵构建低秩更新，并采用零初始化的升维矩阵以保证训练稳定性。\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
