{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37f18ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.2 BERT的自注意力机制与掩码任务\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "659f58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制实现\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_size // self.heads\n",
    "        assert self.head_dim * self.heads == embedding_size, \"embedding_size必须是heads的整数倍\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.head_dim * self.heads, embedding_size)\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, query: torch.Tensor, mask: torch.Tensor = None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # 分割embedding_size到多个头\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)      # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, head_dim)\n",
    "        # 计算注意力分数\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / (self.head_dim ** 0.5)  # (N, heads, query_len, key_len)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)  # (N, heads, query_len, key_len)\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )  # (N, query_len, heads * head_dim)\n",
    "        out = self.fc_out(out)  # (N, query_len, embedding_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 掩码任务数据集创建函数\n",
    "def create_masked_data(inputs, mask_token_id, vocab_size, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, 输入的token ID序列, shape: [batch_size, seq_len]\n",
    "        mask_token_id: int, 用于掩码的特殊token ID\n",
    "        vocab_size: int, 词汇表大小\n",
    "        mask_prob: float, 掩码的概率\n",
    "    Returns:\n",
    "        masked_inputs: torch.Tensor, 掩码后的输入序列\n",
    "        labels: torch.Tensor, 真实标签序列, 未掩码位置为-100\n",
    "    \"\"\"\n",
    "    inputs_with_masks = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    batch_size, seq_len = inputs.shape[:2]\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            prob = random.random()\n",
    "            if prob < mask_prob:\n",
    "                prob /= mask_prob\n",
    "                if prob < 0.8:\n",
    "                    inputs_with_masks[i, j] = mask_token_id  # 80%概率替换为掩码token\n",
    "                elif prob < 0.9:\n",
    "                    inputs_with_masks[i, j] = random.randint(0, vocab_size - 1)  # 10%概率替换为随机token\n",
    "                # 10%概率保持原token不变\n",
    "            else:\n",
    "                labels[i, j] = -100  # 非掩码位置标签设为-100\n",
    "    return inputs_with_masks, labels\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size: int, forward_expansion: int, dropout: float=0.1):\n",
    "        \"\"\"Args: \n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size * forward_expansion)\n",
    "        self.fc2 = nn.Linear(embedding_size * forward_expansion, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Args:\n",
    "            x: Tensor of shape (N, seq_len, embedding_size)\n",
    "        Returns:\n",
    "            out: Tensor of shape (N, seq_len, embedding_size)\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int, forward_expansion: int, dropout: float=0.1):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embedding_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.feed_forward = FeedForward(embedding_size, forward_expansion, dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "        out = self.norm1(attention + x)\n",
    "        dropout_1 = self.dropout(out)\n",
    "        out = self.feed_forward(dropout_1)\n",
    "        out = self.norm2(out + dropout_1)\n",
    "        dropout_2 = self.dropout(out)\n",
    "        return dropout_2\n",
    "\n",
    "# BERT编码器模块\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int, forward_expansion: int, num_layers: int, dropout: float=0.1):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            num_layers: int, Transformer编码器块的数量\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embedding_size, heads, forward_expansion, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cedd574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟数据\n",
    "vocab_size = 30522  # 词汇表大小\n",
    "embedding_size = 768  # BERT-base的嵌入维度\n",
    "num_layers = 12  # Transformer编码器块数量\n",
    "heads = 12  # 注意力头数量\n",
    "forward_expansion = 4  # 前馈网络扩展倍数\n",
    "dropout = 0.1  # dropout概率\n",
    "seq_len = 20  # 序列长度\n",
    "batch_size = 2  # 批量大小\n",
    "mask_token_id = 103  # BERT的掩码token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aca19682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 (带掩码): torch.Size([2, 20, 768]) torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 初始化BERT编码器堆叠\n",
    "bert_encoder = BERTEncoder(embedding_size, heads, forward_expansion, num_layers, dropout).to(device)\n",
    "# 随机生成输入数据和掩码\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # [2, 20]\n",
    "mask_full_one = torch.ones(batch_size, seq_len).to(device)  # [2, 20, 20] 创建一个全1的掩码\n",
    "mask_for_attention = mask_full_one.unsqueeze(1).unsqueeze(2)  # [N, 1, 1, L]\n",
    "# 创建掩码任务数据\n",
    "masked_input_ids, labels = create_masked_data(input_data, mask_token_id, vocab_size)\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_size).to(device)\n",
    "masked_embeddings = embedding_layer(masked_input_ids)  # [2, 20, 768]\n",
    "print(\"输入数据 (带掩码):\", masked_embeddings.shape, mask_full_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9de36f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT编码器输出形状: torch.Size([2, 20, 768])\n",
      "掩码后的输入: tensor([[[ 0.6669,  1.1187, -1.0041,  ..., -0.2817, -0.4558,  1.9066],\n",
      "         [-1.1914, -0.9554,  0.7611,  ...,  0.8293,  0.1727, -0.4522],\n",
      "         [ 1.9041,  0.0209, -1.4513,  ..., -0.2897,  0.1818, -1.7809],\n",
      "         ...,\n",
      "         [-0.7646, -0.6862, -2.3349,  ..., -0.3386, -1.5757, -0.1685],\n",
      "         [-1.6908,  0.6005, -0.6600,  ..., -0.3241,  1.2650,  0.1150],\n",
      "         [ 1.0049,  2.0781,  1.2398,  ..., -1.5259, -1.6640, -0.0551]],\n",
      "\n",
      "        [[-0.1353, -1.4016, -0.0999,  ..., -0.1649,  0.0735, -0.6180],\n",
      "         [-0.2818,  0.6535, -0.4367,  ...,  0.0062, -1.7736,  0.7129],\n",
      "         [-0.7647,  0.8749,  0.3230,  ..., -0.2911,  0.2479, -0.7094],\n",
      "         ...,\n",
      "         [-1.1440,  0.8340, -2.2205,  ..., -0.8262,  1.8734, -0.8777],\n",
      "         [-0.1865,  0.0545,  0.8164,  ..., -0.1453, -0.7726,  0.1557],\n",
      "         [-1.1914, -0.9554,  0.7611,  ...,  0.8293,  0.1727, -0.4522]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "掩码标签: tensor([[ -100, 10166,  -100, 22637,  -100,  -100,  -100,  -100, 13449,  -100,\n",
      "          -100,  -100,  -100,  8714,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 29973,  -100,  -100,  -100,  -100,  -100,  -100,  1060]],\n",
      "       device='cuda:0')\n",
      "编码器输出: tensor([[[-0.5603,  2.6672, -0.0094,  ...,  0.2388, -0.1291, -0.1061],\n",
      "         [ 0.0000, -0.4458, -0.0942,  ..., -0.4195, -0.3368, -0.7831],\n",
      "         [-0.9134,  0.0000,  0.3145,  ..., -0.3034, -0.4087, -0.1525],\n",
      "         ...,\n",
      "         [-0.8284,  0.9917, -0.1507,  ..., -0.2200,  0.3568, -0.0672],\n",
      "         [-2.3988,  0.5682,  0.6592,  ..., -0.6470,  0.6124,  0.2026],\n",
      "         [ 0.7551, -0.0144, -0.2821,  ..., -0.2855, -1.3451, -0.9152]],\n",
      "\n",
      "        [[-0.2691, -0.7772,  2.6885,  ...,  0.9115,  0.7260,  0.4765],\n",
      "         [-0.3304, -0.1210, -1.1724,  ..., -1.0323,  0.0953,  3.0221],\n",
      "         [ 0.5674,  0.0119, -0.0000,  ...,  0.6049,  0.0162, -0.5232],\n",
      "         ...,\n",
      "         [-0.8217,  0.4546,  2.2757,  ..., -0.4767,  0.9209,  0.1822],\n",
      "         [ 0.2215, -0.0733,  5.8497,  ..., -0.0220, -2.2355,  0.1631],\n",
      "         [ 0.2502, -0.9808,  1.5831,  ..., -0.3663,  0.5093, -0.0687]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output = bert_encoder(masked_embeddings, mask=None)\n",
    "print(\"BERT编码器输出形状:\", output.shape)  # 应为 [2, 20, 768]\n",
    "print(\"掩码后的输入:\", masked_embeddings)\n",
    "print(\"掩码标签:\", labels)\n",
    "print(\"编码器输出:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
