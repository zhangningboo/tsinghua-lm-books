{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f18ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1.2 BERT的自注意力机制与掩码任务\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659f58c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制实现\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_size // self.heads\n",
    "        assert self.head_dim * self.heads == embedding_size, \"embedding_size必须是heads的整数倍\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(self.head_dim * self.heads, embedding_size)\n",
    "\n",
    "    def forward(self, values: torch.Tensor, keys: torch.Tensor, query: torch.Tensor, mask: torch.Tensor = None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # 分割embedding_size到多个头\n",
    "        values = values.view(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.view(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.view(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)      # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, head_dim)\n",
    "        # 计算注意力分数\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / (self.head_dim ** 0.5)  # (N, heads, query_len, key_len)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = torch.softmax(energy, dim=-1)  # (N, heads, query_len, key_len)\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )  # (N, query_len, heads * head_dim)\n",
    "        out = self.fc_out(out)  # (N, query_len, embedding_size)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 掩码任务数据集创建函数\n",
    "def create_masked_data(inputs, mask_token_id, vocab_size, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, 输入的token ID序列, shape: [batch_size, seq_len]\n",
    "        mask_token_id: int, 用于掩码的特殊token ID\n",
    "        vocab_size: int, 词汇表大小\n",
    "        mask_prob: float, 掩码的概率\n",
    "    Returns:\n",
    "        masked_inputs: torch.Tensor, 掩码后的输入序列\n",
    "        labels: torch.Tensor, 真实标签序列, 未掩码位置为-100\n",
    "    \"\"\"\n",
    "    inputs_with_masks = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    batch_size, seq_len = inputs.shape[:2]\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            prob = random.random()\n",
    "            if prob < mask_prob:\n",
    "                prob /= mask_prob\n",
    "                if prob < 0.8:\n",
    "                    inputs_with_masks[i, j] = mask_token_id  # 80%概率替换为掩码token\n",
    "                elif prob < 0.9:\n",
    "                    inputs_with_masks[i, j] = random.randint(0, vocab_size - 1)  # 10%概率替换为随机token\n",
    "                # 10%概率保持原token不变\n",
    "            else:\n",
    "                labels[i, j] = -100  # 非掩码位置标签设为-100\n",
    "    return inputs_with_masks, labels\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size: int, forward_expansion: int, dropout: float=0.1):\n",
    "        \"\"\"Args: \n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.fc1 = nn.Linear(embedding_size, embedding_size * forward_expansion)\n",
    "        self.fc2 = nn.Linear(embedding_size * forward_expansion, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"Args:\n",
    "            x: Tensor of shape (N, seq_len, embedding_size)\n",
    "        Returns:\n",
    "            out: Tensor of shape (N, seq_len, embedding_size)\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int, forward_expansion: int, dropout: float=0.1):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embedding_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.feed_forward = FeedForward(embedding_size, forward_expansion, dropout)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "        out = self.norm1(attention + x)\n",
    "        dropout_1 = self.dropout(out)\n",
    "        out = self.feed_forward(dropout_1)\n",
    "        out = self.norm2(out + dropout_1)\n",
    "        dropout_2 = self.dropout(out)\n",
    "        return dropout_2\n",
    "\n",
    "# BERT编码器模块\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int, forward_expansion: int, num_layers: int, dropout: float=0.1):\n",
    "        \"\"\"Args:\n",
    "            embedding_size: int, 输入和输出的维度大小\n",
    "            heads: int, 注意力头的数量\n",
    "            forward_expansion: int, 前馈网络中间层的扩展倍数\n",
    "            num_layers: int, Transformer编码器块的数量\n",
    "            dropout: float, dropout概率\n",
    "        \"\"\"\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embedding_size, heads, forward_expansion, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cedd574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟数据\n",
    "vocab_size = 30522  # 词汇表大小\n",
    "embedding_size = 768  # BERT-base的嵌入维度\n",
    "num_layers = 12  # Transformer编码器块数量\n",
    "heads = 12  # 注意力头数量\n",
    "forward_expansion = 4  # 前馈网络扩展倍数\n",
    "dropout = 0.1  # dropout概率\n",
    "seq_len = 20  # 序列长度\n",
    "batch_size = 2  # 批量大小\n",
    "mask_token_id = 103  # BERT的掩码token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca19682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入数据 (带掩码): torch.Size([2, 20, 768]) torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 初始化BERT编码器堆叠\n",
    "bert_encoder = BERTEncoder(embedding_size, heads, forward_expansion, num_layers, dropout).to(device)\n",
    "# 随机生成输入数据和掩码\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)  # [2, 20]\n",
    "mask_full_one = torch.ones(batch_size, seq_len).to(device)  # [2, 20, 20] 创建一个全1的掩码\n",
    "mask_for_attention = mask_full_one.unsqueeze(1).unsqueeze(2)  # [N, 1, 1, L]\n",
    "# 创建掩码任务数据\n",
    "masked_input_ids, labels = create_masked_data(input_data, mask_token_id, vocab_size)\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_size).to(device)\n",
    "masked_embeddings = embedding_layer(masked_input_ids.to(device))  # [2, 20, 768]\n",
    "print(\"输入数据 (带掩码):\", masked_embeddings.shape, mask_full_one.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de36f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT编码器输出形状: torch.Size([2, 20, 768])\n",
      "掩码后的输入: tensor([[[-0.0894,  2.0584,  1.0405,  ..., -1.5167, -0.0473,  0.8926],\n",
      "         [ 0.3969,  0.2182, -1.6347,  ...,  1.3943, -0.5686, -0.3926],\n",
      "         [-1.0851,  1.7732, -0.9555,  ..., -0.9165,  2.1876, -0.9880],\n",
      "         ...,\n",
      "         [ 0.7479, -0.9750, -0.8472,  ...,  0.0986, -1.3747,  0.6815],\n",
      "         [ 0.0255, -0.3142,  0.6150,  ...,  1.0225, -1.2544, -1.3327],\n",
      "         [-1.0851,  1.7732, -0.9555,  ..., -0.9165,  2.1876, -0.9880]],\n",
      "\n",
      "        [[-1.0851,  1.7732, -0.9555,  ..., -0.9165,  2.1876, -0.9880],\n",
      "         [-1.0902,  0.8716, -1.1871,  ...,  0.6445,  1.2657, -1.9303],\n",
      "         [ 0.0322,  0.6626,  0.1693,  ..., -0.3686, -0.8129, -0.4271],\n",
      "         ...,\n",
      "         [-0.6271, -0.7177, -0.7554,  ..., -1.5640, -0.4752, -0.1398],\n",
      "         [ 0.4356, -0.2283,  1.2764,  ...,  1.1786, -0.5098, -0.6045],\n",
      "         [ 0.2897,  0.4171, -0.6501,  ..., -1.4326, -0.1282, -0.9770]]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "掩码标签: tensor([[ -100,  -100, 22668,  -100,  -100,  7597,  -100,  -100,  -100,  -100,\n",
      "         17516,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 15837],\n",
      "        [15601,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  8565,  -100,\n",
      "          -100, 17634,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]],\n",
      "       device='cuda:0')\n",
      "编码器输出: tensor([[[-0.0348,  0.7583, -0.1313,  ...,  0.2967, -0.4577, -0.6212],\n",
      "         [ 1.2765,  0.0000, -0.2946,  ...,  0.7075,  0.1956, -0.0000],\n",
      "         [ 0.4696,  0.9954, -0.2551,  ...,  0.1233,  0.0000, -0.0347],\n",
      "         ...,\n",
      "         [-0.0365, -0.3975,  0.8471,  ...,  0.3890, -0.4200, -0.0107],\n",
      "         [ 0.3500,  0.7384,  0.2853,  ...,  4.4938,  2.0129,  0.1893],\n",
      "         [ 0.0000,  3.3087, -0.3352,  ...,  0.0000, -0.5466,  0.1974]],\n",
      "\n",
      "        [[ 0.5960, -0.0000, -0.2736,  ...,  0.5217,  0.0000, -1.4399],\n",
      "         [ 0.7904,  1.5782,  0.3355,  ..., -0.0000,  0.1927,  0.1628],\n",
      "         [ 0.2972,  0.3803, -0.1672,  ..., -0.3593,  0.1196, -0.8058],\n",
      "         ...,\n",
      "         [ 0.2984, -0.0000,  1.0498,  ..., -0.7055, -0.1059, -0.0000],\n",
      "         [-0.0000, -0.0198,  0.6820,  ...,  0.1820,  1.1526,  0.0746],\n",
      "         [ 1.3966, -0.0293, -0.2842,  ..., -0.7199,  0.3887,  0.2733]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output = bert_encoder(masked_embeddings, mask=None)\n",
    "print(\"BERT编码器输出形状:\", output.shape)  # 应为 [2, 20, 768]\n",
    "print(\"掩码后的输入:\", masked_embeddings)\n",
    "print(\"掩码标签:\", labels)\n",
    "print(\"编码器输出:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
