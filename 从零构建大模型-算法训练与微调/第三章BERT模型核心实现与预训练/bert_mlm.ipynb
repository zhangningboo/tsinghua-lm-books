{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d40ff85",
   "metadata": {},
   "source": [
    "#### 掩码语言模型MLM： 是BERT模型的核心预训练任务。通过在输入文本中随机遮掩部分词语并让模型预测这些词，模型能够有效地学习词的上下文语义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21860de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93cad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力模块\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: 嵌入向量的维度\n",
    "            heads: 注意力头的数量\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embedding_size // heads\n",
    "        assert (\n",
    "            self.head_dim * heads == embedding_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            values: shape (N, value_len, embedding_size)\n",
    "            keys: shape (N, key_len, embedding_size)\n",
    "            query: shape (N, query_len, embedding_size)\n",
    "            mask: shape (N, 1, query_len, key_len)\n",
    "        Returns:\n",
    "            out: shape (N, query_len, embedding_size)\n",
    "        \"\"\"\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # 分割嵌入向量为多个头\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / (self.embedding_size ** 0.5)\n",
    "\n",
    "        # 调整mask的形状\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # (N, 1, 1, seq_len)\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        out = torch.einsum(\"nhqk,nkhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# 前馈神经网络模块\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_size: int, forward_expansion: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: 嵌入向量的维度\n",
    "            forward_expansion: 前馈网络中间层的扩展倍数\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embedding_size, forward_expansion * embedding_size)\n",
    "        self.fc2 = nn.Linear(forward_expansion * embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (N, seq_len, embedding_size)\n",
    "        Returns:\n",
    "            out: shape (N, seq_len, embedding_size)\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Transformer编码器块\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size: int,\n",
    "        heads: int,\n",
    "        forward_expansion: int,\n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: 嵌入向量的维度\n",
    "            heads: 注意力头的数量\n",
    "            forward_expansion: 前馈网络中间层的扩展倍数\n",
    "            dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embedding_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embedding_size)\n",
    "        self.norm2 = nn.LayerNorm(embedding_size)\n",
    "        self.feed_forward = FeedForward(embedding_size, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            value: shape (N, value_len, embedding_size)\n",
    "            key: shape (N, key_len, embedding_size)\n",
    "            query: shape (N, query_len, embedding_size)\n",
    "            mask: shape (N, 1, query_len, key_len)\n",
    "        Returns:\n",
    "            out: shape (N, query_len, embedding_size)\n",
    "        \"\"\"\n",
    "        attention = self.attention(x, x, x, mask)\n",
    "        norm1_out = self.norm1(attention + x)\n",
    "        # 残差连接和层归一化\n",
    "        drop1_out = self.dropout(norm1_out)\n",
    "        forward = self.feed_forward(drop1_out)\n",
    "        norm2_out = self.norm2(forward + drop1_out)\n",
    "        drop2_out = self.dropout(norm2_out)\n",
    "        return drop2_out\n",
    "    \n",
    "\n",
    "# BERT编码器\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size: int, heads: int, forward_expansion: int, num_layers: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: 嵌入向量的维度\n",
    "            heads: 注意力头的数量\n",
    "            forward_expansion: 前馈网络中间层的扩展倍数\n",
    "            num_layers: Transformer块的数量\n",
    "            dropout: dropout概率\n",
    "        \"\"\"\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embedding_size,\n",
    "                    heads,\n",
    "                    forward_expansion,\n",
    "                    dropout,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (N, seq_len, embedding_size)\n",
    "            mask: shape (N, 1, seq_len, seq_len)\n",
    "        Returns:\n",
    "            out: shape (N, seq_len, embedding_size)\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4b49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建掩码任务数据\n",
    "def create_masked_lm_data(inputs, vocab_size, mask_token_id, pad_token_id, mlm_probability=0.15):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: shape (N, seq_len)\n",
    "        vocab_size: 词汇表大小\n",
    "        mask_token_id: 掩码标记的ID\n",
    "        pad_token_id: 填充标记的ID\n",
    "        mlm_probability: 掩码概率\n",
    "    Returns:\n",
    "        inputs_masked: shape (N, seq_len)\n",
    "        labels: shape (N, seq_len)\n",
    "    \"\"\"\n",
    "    inputs_with_masks = inputs.clone()\n",
    "    labels = inputs.clone()\n",
    "    for i in range(inputs.shape[0]):\n",
    "        for j in range(inputs.shape[1]):\n",
    "            if inputs[i, j] == pad_token_id:\n",
    "                labels[i, j] = -100  # 不计算填充位置的损失\n",
    "                continue\n",
    "            prob = random.random()\n",
    "            if prob < mlm_probability:\n",
    "                prob /= mlm_probability\n",
    "                # 80%的时间替换为掩码标记\n",
    "                if prob < 0.8:\n",
    "                    inputs_with_masks[i, j] = mask_token_id\n",
    "                # 10%的时间替换为随机标记\n",
    "                elif prob < 0.9:\n",
    "                    inputs_with_masks[i, j] = random.randint(0, vocab_size - 1)\n",
    "                # 剩下的10%保持不变\n",
    "                # labels已经是原始输入，无需修改\n",
    "            else:\n",
    "                labels[i, j] = -100  # 只计算掩码位置的损失\n",
    "    return inputs_with_masks, labels\n",
    "\n",
    "    # labels = inputs.clone()\n",
    "    # probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    # special_tokens_mask = (inputs == pad_token_id)\n",
    "    # probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    # masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    # labels[~masked_indices] = -100  # 只计算掩码位置的损失\n",
    "\n",
    "    # # 80%的时间替换为掩码标记\n",
    "    # indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    # inputs[indices_replaced] = mask_token_id\n",
    "\n",
    "    # # 10%的时间替换为随机标记\n",
    "    # indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    # random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long)\n",
    "    # inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # # 剩下的10%保持不变\n",
    "\n",
    "    # return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ab3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM任务实现\n",
    "class MLMTask(nn.Module):\n",
    "    def __init__(self, embedding_size: int, vocab_size: int, heads: int, forward_expansion: int, num_layers: int, dropout: float=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: 嵌入向量的维度\n",
    "            vocab_size: 词汇表大小\n",
    "        \"\"\"\n",
    "        super(MLMTask, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.bert_encoder = BERTEncoder(\n",
    "            embedding_size=embedding_size,\n",
    "            heads=heads,\n",
    "            forward_expansion=forward_expansion,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (N, seq_len, embedding_size)\n",
    "            mask: shape (N, 1, seq_len, seq_len)\n",
    "        Returns:\n",
    "            out: shape (N, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(x)\n",
    "        encorder_output = self.bert_encoder(embeddings, mask)\n",
    "        logits = self.fc(encorder_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d80a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟数据\n",
    "vocab_size = 30522\n",
    "embedding_size = 768\n",
    "num_layers = 12\n",
    "heads = 12\n",
    "forward_expansion = 4\n",
    "dropout = 0.1\n",
    "mask_token_id = 103\n",
    "pad_token_id = 0\n",
    "sequence_length = 128\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16a8964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9064, 21771,  5708,  1027, 15083],\n",
       "        [ 6383, 10097, 22037,  7989, 15117]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, vocab_size, (2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce1e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.618671417236328\n",
      "模型输出形状: torch.Size([2, 128, 30522])\n",
      "标签形状: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "# 初始化MLM任务模型\n",
    "mlm_model = MLMTask(\n",
    "    embedding_size=embedding_size,\n",
    "    vocab_size=vocab_size,\n",
    "    heads=heads,\n",
    "    forward_expansion=forward_expansion,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "# 模拟输入数据\n",
    "inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))  # [2, 20]\n",
    "# 创建掩码任务数据\n",
    "mask_for_attention = torch.ones((batch_size, sequence_length)) # 全部位置都可见\n",
    "inputs_masked, labels = create_masked_lm_data(inputs, vocab_size, mask_token_id, pad_token_id)\n",
    "# 前向传播\n",
    "logits = mlm_model(inputs_masked, mask_for_attention)\n",
    "# 计算损失\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, vocab_size), labels.view(-1))\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "print(\"模型输出形状:\", logits.shape)  # 应为 (batch_size, sequence_length, vocab_size)\n",
    "print(\"标签形状:\", labels.shape)  # 应为 (batch_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "377a3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 14:57:04,870 - modelscope - INFO - Not logged-in, you can login for uploadingor accessing controlled entities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /data/nvme1n1p1/zhangningboo/workspace/tsinghua-lm-books/从零构建大模型-算法训练与微调/第三章BERT模型核心实现与预训练/bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('google-bert/bert-base-uncased', local_dir='./bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1de4e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 202/202 [00:00<00:00, 411.44it/s, Materializing param=cls.predictions.transform.dense.weight]                 \n",
      "BertForMaskedLM LOAD REPORT from: ./bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1999, 1996, 6123, 1997, 3795, 3171, 3930, 1010, 1996, 5157, 2005,\n",
      "         2974, 3316, 1005,  103, 2003, 4852, 5901, 1012,  102]])\n",
      "Predicted token: products\n",
      "Predicted token: services\n",
      "Predicted token: technology\n",
      "Predicted token: solutions\n",
      "Predicted token: equipment\n",
      "Predicted tokens for [MASK]: ['products', 'services', 'technology', 'solutions', 'equipment']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('./bert-base-uncased')\n",
    "# 定义带有掩码的句子\n",
    "text = \"In the context of global economic growth, the demand for technology companies' [MASK] is increasing rapidly. \"\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "print(input_ids)\n",
    "\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "# 前向传播预测掩码位置的词汇分布\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "logits = output.logits\n",
    "# 获取掩码位置的预测结果，并取概率最高的词\n",
    "mask_token_logits = logits[0, mask_token_index, :]\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(f\"Predicted token: {tokenizer.decode([token])}\")\n",
    "# 将预测词插入句子中，生成完整的标题\n",
    "predicted_tokens = [tokenizer.decode([token]) for token in top_5_tokens]\n",
    "print(\"Predicted tokens for [MASK]:\", predicted_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
