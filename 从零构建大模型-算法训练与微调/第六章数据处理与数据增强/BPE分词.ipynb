{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cb1a81",
   "metadata": {},
   "source": [
    "分词方法\n",
    "- 词粒度分词\n",
    "- 字节对编码(BPE)分词：合并高频字节对，递归生成新的子词，可有效解决稀有词问题\n",
    "- 基于子词的分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac7588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdab632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例数据\n",
    "data = {\n",
    "    'text': [\n",
    "        \"自然语言处理是人工智能的一个重要领域。\",\n",
    "        \"数据增强技术可以扩展数据集，提高模型的泛化能力。\",\n",
    "        \"深度学习和机器学习是现代人工智能的核心技术。\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff4b997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'哈': 5, '你': 2, '好': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词函数\n",
    "def get_vocab(texts):\n",
    "    vocab = Counter()\n",
    "    for text in texts:\n",
    "        text = ' '.join(list(text)) # 按字符切分\n",
    "        vocab.update(text.split())\n",
    "    return vocab\n",
    "\n",
    "\n",
    "get_vocab([\"你好你\", \"哈哈哈哈哈\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE分词合并\n",
    "def merge_vocab(vocab, pair):\n",
    "    new_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in vocab:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        new_vocab[w_out] = vocab[word]\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最频的字对\n",
    "def get_stats(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede90de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE分词主函数\n",
    "def bpe_tokenize(texts, num_merges):\n",
    "    vocab = get_vocab(texts)\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(vocab, best)\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
