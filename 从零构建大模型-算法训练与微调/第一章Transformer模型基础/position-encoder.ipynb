{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407296df",
   "metadata": {},
   "source": [
    "位置编码器\n",
    "  - transformer处理信息时无序\n",
    "  - 为引入位置信息使用\n",
    "  - 使模型能够识别不同词之间的相对位置关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26e96f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建位置编码矩阵\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * (-math.log(10000.0) / embedding_size))\n",
    "        pe = torch.zeros(max_len, embedding_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # 添加批次维度\n",
    "        self.register_buffer('pe', pe) # 将位置编码注册为缓冲区,不作为模型参数更新\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将位置编码添加到输入嵌入中\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "769a2b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " torch.Size([10, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 10\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "position, position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b581291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10000.0010)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor(math.log(10000.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf76ae",
   "metadata": {},
   "source": [
    "$$ \\text{div\\_term}_i = 10000^{2i / d_{\\text{model}}}  \\\\\n",
    "    math.log(10000.0) = \\ln10000.0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8152be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带位置编码的嵌入层\n",
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_len=5000):\n",
    "        super(EmbeddingWithPositionalEncoding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_size, max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # * math.sqrt(self.embedding.embedding_dim)  # 缩放嵌入\n",
    "        x = self.pos_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b31351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "vocab_size = 100  # 词汇表大小\n",
    "embedding_size = 64  # 嵌入维度\n",
    "max_len = 50  # 最大序列长度\n",
    "batch_size = 2  # 批量大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69ad14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 64])\n",
      "位置编码后的嵌入输出: tensor([[[-1.0561,  1.0751, -1.3608,  ...,  1.2327,  0.1943, -0.2491],\n",
      "         [ 0.6463, -0.5637,  1.2275,  ...,  0.2415, -0.4657,  0.0312],\n",
      "         [ 1.3585, -1.6629,  0.6527,  ...,  1.3106, -1.4897,  0.2261],\n",
      "         ...,\n",
      "         [ 0.4542, -0.4600, -1.7503,  ...,  2.2198, -0.1154,  1.9548],\n",
      "         [-1.3759, -1.7832, -2.2850,  ...,  0.3963, -0.0343,  0.2413],\n",
      "         [ 0.2892, -0.0244, -1.7518,  ...,  2.2570,  0.4946,  2.2084]],\n",
      "\n",
      "        [[ 0.0775,  2.0395, -0.9150,  ...,  0.0586, -1.0018, -0.6656],\n",
      "         [ 2.5678,  0.7628,  1.6714,  ...,  0.7785,  0.1843,  1.2955],\n",
      "         [-0.0403, -0.2317,  0.0995,  ...,  0.2330,  0.4641,  1.9797],\n",
      "         ...,\n",
      "         [ 0.0164, -1.0728, -1.1679,  ..., -0.1144,  0.1658,  1.6534],\n",
      "         [-1.3759, -1.7832, -2.2850,  ...,  0.3963, -0.0343,  0.2413],\n",
      "         [-2.5312,  1.1633, -1.8766,  ...,  1.9803,  1.7273,  0.3746]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "位置编码矩阵的一部分: tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,\n",
      "           1.0000,  0.0000,  1.0000],\n",
      "         [ 0.8415,  0.5403,  0.6816,  0.7318,  0.5332,  0.8460,  0.4093,\n",
      "           0.9124,  0.3110,  0.9504],\n",
      "         [ 0.9093, -0.4161,  0.9975,  0.0709,  0.9021,  0.4315,  0.7469,\n",
      "           0.6649,  0.5911,  0.8066],\n",
      "         [ 0.1411, -0.9900,  0.7783, -0.6279,  0.9933, -0.1160,  0.9536,\n",
      "           0.3010,  0.8126,  0.5828],\n",
      "         [-0.7568, -0.6536,  0.1415, -0.9899,  0.7785, -0.6277,  0.9933,\n",
      "          -0.1157,  0.9536,  0.3011]]])\n"
     ]
    }
   ],
   "source": [
    "# 模拟输入数据\n",
    "input_data = torch.randint(0, vocab_size, (batch_size, max_len))  # 随机整数作为词索引\n",
    "# 初始化带位置编码的嵌入层\n",
    "embedding_layer = EmbeddingWithPositionalEncoding(vocab_size, embedding_size, max_len)\n",
    "# 前向传播\n",
    "output = embedding_layer(input_data)\n",
    "print(output.shape)  # 输出形状应为 (batch_size, max_len, embedding_size)\n",
    "print(\"位置编码后的嵌入输出:\", output)\n",
    "# 检查位置编码的具体值\n",
    "print(\"位置编码矩阵的一部分:\", embedding_layer.pos_encoder.pe[:, :5, :10])  # 打印前5个位置的前10维编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9b2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据的嵌入输出: torch.Size([2, 40, 64])\n",
      "测试数据的嵌入输出: tensor([[[ 1.1289,  1.9779,  0.9760,  ...,  2.6218, -0.0465,  2.3552],\n",
      "         [-0.2126,  0.3550,  0.5644,  ..., -0.3758,  0.0059, -1.0395],\n",
      "         [-0.9947, -0.2407,  1.2984,  ...,  0.6317,  0.4573,  0.7703],\n",
      "         ...,\n",
      "         [-0.5170,  1.5539, -0.4908,  ...,  0.4382,  0.3909,  1.3073],\n",
      "         [ 0.6961, -0.5537,  1.0388,  ...,  1.9344, -0.9189,  1.0266],\n",
      "         [ 0.5713, -0.0043, -0.5297,  ...,  2.0219,  0.0780,  0.0214]],\n",
      "\n",
      "        [[-1.9113,  0.4304,  0.3782,  ...,  1.7448, -0.8705,  2.3921],\n",
      "         [ 1.7945,  0.4640,  1.6173,  ...,  2.4166, -0.7445,  0.7879],\n",
      "         [-1.2218, -0.4883,  1.2362,  ...,  2.5640,  0.7730,  2.5356],\n",
      "         ...,\n",
      "         [ 0.5144,  1.6762, -0.0917,  ...,  1.3068, -1.4846,  1.4938],\n",
      "         [-1.2811,  1.8178, -1.2805,  ...,  1.9803,  1.7258,  0.3746],\n",
      "         [ 1.6554, -0.2844,  1.0370,  ...,  1.0341,  1.3276,  2.6463]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 测试位置编码在不同序列长度下的稳定性\n",
    "test_input = torch.randint(0, vocab_size, (batch_size, max_len - 10))  # 更长的序列\n",
    "test_output = embedding_layer(test_input)\n",
    "print(\"测试数据的嵌入输出:\", test_output.shape)\n",
    "print(\"测试数据的嵌入输出:\", test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frome_zero_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
