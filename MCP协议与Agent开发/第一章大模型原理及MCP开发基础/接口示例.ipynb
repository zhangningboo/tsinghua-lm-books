{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c0221b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.deepseek.com'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "base_url = os.getenv(\"BASE_URL\")\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ceffc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='fa7d2d88-19e4-4c50-a246-4518319ce1e9', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='好的，Transformer 的核心机制可以概括为：**一种完全基于自注意力机制和前馈神经网络，摒弃了循环和卷积结构，用于处理序列数据的模型架构。**\\n\\n它的核心思想是：**让序列中的每一个元素（如一个词）都能够直接“看到”并聚合整个序列中所有其他元素的信息**，从而进行高效的并行计算和长距离依赖建模。\\n\\n下面我们来分解其最核心的几个机制：\\n\\n### 1. 自注意力机制\\n这是 Transformer 的灵魂。它的目的是计算序列中每个元素与其他所有元素的“关联度”（注意力分数），然后根据这个关联度对所有元素的信息进行加权求和，从而得到该元素新的、包含全局信息的表示。\\n\\n**计算过程（以“我 爱 你”这句话为例）：**\\n1.  **生成三个向量**：对于输入序列中的每个词（如“爱”），通过可学习的权重矩阵，生成三个向量：\\n    *   **查询向量**：代表“我要寻找什么信息”。\\n    *   **键向量**：代表“我拥有什么信息，可供匹配”。\\n    *   **值向量**：代表“我实际要提供的信息”。\\n2.  **计算注意力分数**：用“爱”的 **Q** 去和序列中所有词（包括“爱”自己）的 **K** 做点积，得到一组分数。分数越高，表示“爱”与那个词在当前任务下的关联性越强。\\n    *   例如，“爱”的 Q 与“我”的 K 点积可能得到一个分数，与“你”的 K 点积得到另一个分数。\\n3.  **缩放与归一化**：将分数除以一个缩放因子（通常是键向量维度的平方根），然后通过 Softmax 函数进行归一化，得到一组权重（和为1）。这确保了梯度的稳定性。\\n4.  **加权求和**：用上一步得到的权重，对序列中所有词的 **V** 进行加权求和。最终得到的向量就是“爱”这个词经过自注意力机制处理后的新表示。**这个新表示融合了它与整个句子中所有词的关系信息。**\\n\\n**多头注意力**：为了捕捉不同种类的关系（例如语法关系、语义关系），Transformer 会将这个过程并行执行多次（即多个“头”），每个头有不同的 Q、K、V 投影矩阵，最后将多个头的结果拼接起来再投影。这就像用多个不同的“视角”来审视同一组数据。\\n\\n### 2. 位置编码\\n由于自注意力机制本身是“无序的”（它对输入序列的排列顺序不敏感），而语言等序列数据顺序至关重要。因此，Transformer 需要显式地注入位置信息。\\n\\n*   **方法**：为序列中每个位置的索引（如第1个词、第2个词）计算一个独一无二的向量（位置编码向量）。\\n*   **特点**：这个向量通常使用正弦和余弦函数生成，具有固定的模式，可以让模型轻松地学习到“相对位置”关系（例如，“距离为5”这种模式）。然后将这个位置编码向量直接加到对应词的输入嵌入向量上。\\n\\n### 3. 编码器-解码器架构\\n标准的 Transformer 模型遵循编码器-解码器结构：\\n\\n*   **编码器**：\\n    *   由 N 个（如6层）相同的层堆叠而成。\\n    *   每一层包含两个核心子层：\\n        1.  **多头自注意力层**：让该层每个位置都能关注输入序列的所有位置。\\n        2.  **前馈神经网络层**：一个简单的全连接网络，独立作用于每个位置。\\n    *   每个子层周围都有 **残差连接** 和 **层归一化**。这是稳定深层网络训练的关键技术。\\n    *   **编码器的任务**：将输入序列（如源语言句子）转换为一组富含上下文信息的“中间表示”。\\n\\n*   **解码器**：\\n    *   同样由 N 个相同的层堆叠而成。\\n    *   每一层包含 **三个** 核心子层：\\n        1.  **掩码多头自注意力层**：**这是关键区别**。为了让解码器在训练时不能“偷看”未来的信息（即只能基于已生成的部分进行预测），这里使用了一个掩码，将当前位置之后的所有位置都遮盖掉（设为负无穷）。\\n        2.  **编码器-解码器注意力层**：这一层的 **Q** 来自解码器的上一子层，而 **K** 和 **V** 来自**编码器的最终输出**。这允许解码器的每个位置都关注输入序列的所有位置，从而实现“对齐”功能（例如在翻译中，生成某个目标词时关注源句中相关的词）。\\n        3.  **前馈神经网络层**：与编码器相同。\\n    *   同样，每个子层都有残差连接和层归一化。\\n    *   **解码器的任务**：基于编码器的输出和已生成的部分序列，自回归地（一个接一个）生成输出序列（如目标语言句子）。\\n\\n### 核心优势总结\\n\\n1.  **强大的长距离依赖建模**：自注意力机制一步到位地连接了序列中任意距离的两个位置，解决了 RNN 中的梯度消失/爆炸问题。\\n2.  **极高的并行计算能力**：序列中所有元素的自注意力计算可以同时进行，训练速度远快于必须顺序计算的 RNN。\\n3.  **可解释性**：注意力权重矩阵可以可视化，展示模型在做决策时关注了输入的哪些部分（例如，翻译时某个词对应了源句的哪些词）。\\n\\n正是这些核心机制，使得 Transformer 不仅在机器翻译上取得突破，更成为了当今几乎所有大语言模型（如 GPT、BERT 等）以及多模态模型的基石架构。', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1770643169, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_eaab8d114b_prod0820_fp8_kvcache', usage=CompletionUsage(completion_tokens=1205, prompt_tokens=17, total_tokens=1222, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=17))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat接口\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"你是一个大模型专业的AI助手\" },\n",
    "        { \"role\": \"user\", \"content\": \"请解释一下Transformer的核心机制\" },\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4bb0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力机制的核心思想是通过动态分配权重，让模型在处理信息时能够聚焦于最相关的部分。以下是其核心思想的分点简述：\n",
      "\n",
      "1. **选择性聚焦**  \n",
      "   模仿人类注意力，从大量输入信息中筛选出关键部分，忽略无关或次要内容，提高信息处理效率。\n",
      "\n",
      "2. **权重动态分配**  \n",
      "   通过计算查询（Query）与键（Key）的相似度，生成注意力权重，值（Value）根据权重加权聚合，实现动态重要性调整。\n",
      "\n",
      "3. **上下文感知**  \n",
      "   结合输入序列的全局信息，为每个位置生成上下文相关的表示，克服传统序列模型（如RNN）的长距离依赖问题。\n",
      "\n",
      "4. **并行化与可扩展性**  \n",
      "   自注意力机制允许同时计算所有位置的关系，支持并行计算，适用于长序列处理，并易于扩展至多头注意力。\n",
      "\n",
      "5. **通用适配性**  \n",
      "   可灵活嵌入不同神经网络架构（如Transformer、CNN、RNN），广泛应用于自然语言处理、计算机视觉、多模态任务等领域。\n",
      "\n",
      "这些思想使注意力机制成为现代深度学习模型（如GPT、BERT）的核心组件，显著提升了模型的表达能力和性能。"
     ]
    }
   ],
   "source": [
    "# 流式响应\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"你是一个大模型专业的AI助手\" },\n",
    "        { \"role\": \"user\", \"content\": \"请分点简述注意力机制的核心思想\" },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447f4043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "我目前无法直接查询实时天气数据。不过，您可以通过以下方式快速获取北京的天气信息：\n",
      "\n",
      "1. **推荐使用**：\n",
      "   - 打开手机天气应用（如苹果天气、墨迹天气等）\n",
      "   - 在搜索引擎（百度/谷歌）直接搜索“北京天气”\n",
      "   - 访问中国天气网（www.weather.com.cn）\n",
      "\n",
      "2. **当前季节提示**（2024年1月）：\n",
      "   - 北京正处于冬季，近期气温通常在-5℃~5℃之间\n",
      "   - 建议关注：空气湿度、风力及空气质量指数（AQI）\n",
      "\n",
      "3. **如需出行建议**：\n",
      "   - 冬季需准备羽绒服、围巾等防寒衣物\n",
      "   - 雾霾天气建议佩戴防护口罩\n",
      "\n",
      "需要我帮您整理其他城市的气候特点或出行贴士吗？\n"
     ]
    }
   ],
   "source": [
    "# 函数调用模式，结构化任务执行能力\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": \"请帮我查询北京的天气\" }\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"查询天气信息\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"城市名称\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "# 函数调用结构\n",
    "print(response.choices[0].message.function_call)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好的，快速排序的时间复杂度分析如下：\\n\\n### 1. 最佳情况和平均情况：O(n log n)\\n*   **最佳情况**：每次选择的基准（pivot）都能将数组**均匀地**分成两个大小大致相等的子数组。\\n    *   在这种情况下，递归树的深度约为 **log₂ n**（以2为底n的对数）。\\n    *   在递归树的每一层，所有子问题的划分操作（`partition`）加起来都需要遍历整个数组，总时间为 **O(n)**。\\n    *   因此，总时间复杂度为 **O(n) * O(log n) = O(n log n)**。\\n*   **平均情况**：对于一个随机排列的输入数组，快速排序的平均时间复杂度'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多轮上下文控制模式\n",
    "from openai import OpenAI\n",
    "\n",
    "history = [\n",
    "    { \"role\": \"system\", \"content\": \"你是一个编程助手\" },\n",
    "    { \"role\": \"user\", \"content\": \"写一个快速排序函数\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"好的，以下是快速排序的python代码...\" },\n",
    "    { \"role\": \"user\", \"content\": \"请说明时间复杂度\" },\n",
    "]\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=history,\n",
    "    temperature=0.5,\n",
    "    max_tokens=150,\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
