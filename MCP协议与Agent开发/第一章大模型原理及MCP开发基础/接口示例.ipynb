{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c0221b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.deepseek.com'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "base_url = os.getenv(\"BASE_URL\")\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceffc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='ea2c15ef-3362-488a-a4c8-a2eef75bba4c', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='好的，Transformer 的核心机制可以概括为：**一种完全基于自注意力机制和前馈神经网络，并行处理整个序列，以捕捉长距离依赖关系的编码器-解码器架构。**\\n\\n它的核心突破在于摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN），解决了序列建模中“长距离依赖”和“训练并行化”两大难题。\\n\\n下面我们来分解其最核心的几个机制：\\n\\n### 1. 自注意力机制\\n这是 Transformer 的灵魂。它的核心思想是：**序列中的每个元素（如一个词）都应该与序列中的所有其他元素进行交互，从而动态地计算出一个能融合全局信息的“新表示”。**\\n\\n**运作过程（以“我爱人工智能”这句话为例）：**\\n1.  **生成三个向量**：对于每个输入词（如“爱”），通过三个不同的权重矩阵，生成三个向量：\\n    *   **查询向量**：代表“我在寻找什么”。\\n    *   **键向量**：代表“我有什么可提供的”。\\n    *   **值向量**：代表“我的实际内容是什么”。\\n2.  **计算注意力分数**：用“爱”的 **Q** 去与序列中所有词（包括“爱”自己）的 **K** 进行点积，得到一组分数。分数越高，表示两个词在当前任务下的关联性越强。\\n    *   例如，“爱”的 Q 与“我”的 K 点积可能得分高（谁在爱？），与“人工智能”的 K 点积得分也可能高（爱什么？）。\\n3.  **归一化与加权求和**：将这些分数通过 Softmax 归一化为概率分布（注意力权重）。然后用这些权重对各个词的 **V** 进行加权求和，得到“爱”这个词最终的**上下文感知表示**。\\n    *   这个新表示不再是一个孤立的“爱”字，而是包含了“**我**”和“**人工智能**”信息的“爱”。\\n\\n**公式表示（缩放点积注意力）：**\\n\\\\[ \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V \\\\]\\n其中，\\\\(\\\\sqrt{d_k}\\\\) 是为了防止点积结果过大导致梯度消失。\\n\\n### 2. 多头注意力\\n单一的注意力机制可能只关注到一种类型的依赖关系（例如语法关系）。为了让模型同时关注来自**不同表示子空间**的信息，Transformer 使用了“多头”注意力。\\n\\n*   **做法**：将 Q、K、V 通过多个不同的线性投影矩阵，投影到多个（例如 8 个）低维空间，在每个子空间里独立进行自注意力计算。\\n*   **结果**：每个“头”可能会学习到不同的模式，例如一个头关注“主谓关系”，另一个头关注“指代关系”。最后将所有头的输出拼接起来，再通过一个线性层融合。\\n*   **好处**：增强了模型的容量和表达能力，使其能够并行捕捉多种类型的依赖关系。\\n\\n### 3. 位置编码\\n由于自注意力机制本身是**置换不变**的（即打乱输入顺序，输出不变），它无法感知序列中元素的顺序信息。而语言中顺序至关重要。\\n\\n*   **解决方案**：在将词向量输入编码器/解码器之前，为其加上一个**位置编码向量**。\\n*   **方法**：Transformer 使用了一组固定公式（正弦和余弦函数）来生成位置编码。不同位置（pos=1，2，3...）有唯一且确定的编码向量。模型通过将词向量与位置编码向量相加，使输入本身就携带了位置信息。\\n\\n### 4. 编码器-解码器架构与掩码注意力\\nTransformer 采用了经典的编码器-解码器结构，但内部全部由注意力机制驱动。\\n\\n*   **编码器**：由 N 个相同的层堆叠而成。每层包含一个**多头自注意力子层**和一个**前馈神经网络子层**，每个子层后都有残差连接和层归一化。编码器的作用是提取输入序列的全局上下文表示。\\n*   **解码器**：同样由 N 个相同的层堆叠。每层包含**三个子层**：\\n    1.  **掩码多头自注意力层**：对已生成的目标序列进行自注意力计算。为了防止模型在训练时“偷看”未来的答案，使用**注意力掩码**将当前位置之后的所有位置屏蔽掉（设为负无穷），确保预测只依赖于已生成的词。\\n    2.  **编码器-解码器注意力层**：这是连接编码器和解码器的关键。它的 **Q** 来自解码器上一层的输出，而 **K** 和 **V** 来自编码器的最终输出。这样，解码器在生成每一个词时，都能有选择地聚焦于输入序列中最相关的部分。\\n    3.  **前馈神经网络层**。\\n*   **输出**：解码器的最终输出通过一个线性层和一个 Softmax 层，转化为下一个词的概率分布。\\n\\n### 核心优势总结\\n1.  **强大的长距离依赖建模**：自注意力机制一步到位地连接了序列中任意两个位置，克服了 RNN 的梯度消失/爆炸问题。\\n2.  **极高的训练并行度**：序列中所有元素的注意力计算可以同时进行，极大提升了 GPU 利用率，训练速度远超 RNN。\\n3.  **可解释性**：注意力权重矩阵可以可视化，让我们看到模型在做决策时“关注”了输入序列的哪些部分。\\n\\n正是这些核心机制，使得 Transformer 成为自然语言处理乃至整个 AI 领域的基石模型，催生了 BERT、GPT、T5 等一系列划时代的模型。', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1770628780, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_eaab8d114b_prod0820_fp8_kvcache', usage=CompletionUsage(completion_tokens=1196, prompt_tokens=17, total_tokens=1213, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=17))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat接口\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"你是一个大模型专业的AI助手\" },\n",
    "        { \"role\": \"user\", \"content\": \"请解释一下Transformer的核心机制\" },\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4bb0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力机制的核心思想是通过动态分配权重，使模型能够有选择地关注输入数据中的关键信息。以下是其核心思想的分点简述：\n",
      "\n",
      "1. **选择性聚焦**  \n",
      "   模仿人类注意力，允许模型在处理信息时忽略无关部分，集中资源于最重要的特征或上下文，提升计算效率。\n",
      "\n",
      "2. **权重动态分配**  \n",
      "   通过计算查询（Query）与键（Key）的相似度，生成注意力权重，再对值（Value）加权求和。权重随输入内容动态调整，实现灵活的信息提取。\n",
      "\n",
      "3. **上下文依赖建模**  \n",
      "   在序列任务（如翻译）中，通过关注不同位置的相关信息，建立长距离依赖关系，解决传统循环神经网络（RNN）的长程信息衰减问题。\n",
      "\n",
      "4. **并行化与可扩展性**  \n",
      "   自注意力机制（如Transformer）可并行计算所有位置的关联，避免序列顺序依赖，显著提升训练速度，并支持处理更长的输入序列。\n",
      "\n",
      "5. **多维度信息融合**  \n",
      "   多头注意力机制允许模型同时关注不同子空间的特征（如语义、语法），增强信息捕获的多样性和表达能力。\n",
      "\n",
      "这些思想使注意力机制成为现代深度学习模型（如Transformer、BERT、GPT）的核心组件，广泛应用于自然语言处理、计算机视觉等领域。"
     ]
    }
   ],
   "source": [
    "# 流式响应\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"你是一个大模型专业的AI助手\" },\n",
    "        { \"role\": \"user\", \"content\": \"请分点简述注意力机制的核心思想\" },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447f4043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "我目前无法直接查询实时天气数据。不过，您可以通过以下方式快速获取北京的天气信息：\n",
      "\n",
      "1. **推荐使用**：\n",
      "   - 打开手机天气App（如系统自带天气、墨迹天气等）\n",
      "   - 在搜索引擎（百度/谷歌）搜索“北京天气”\n",
      "   - 通过微信小程序搜索“天气预报”\n",
      "\n",
      "2. **备用方案**：\n",
      "   - 访问中国天气网（www.weather.com.cn）\n",
      "   - 查看手机通知栏的天气插件\n",
      "\n",
      "如果您需要了解天气相关的出行建议（如穿衣指南、是否需带伞等），可以告诉我您的具体需求，我会根据一般性知识为您提供参考建议。🌤️\n"
     ]
    }
   ],
   "source": [
    "# 函数调用模式，结构化任务执行能力\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": \"请帮我查询北京的天气\" }\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"查询天气信息\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"城市名称\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "# 函数调用结构\n",
    "print(response.choices[0].message.function_call)\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinghua-lm-books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
